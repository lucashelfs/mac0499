{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classificação de tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helfs/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para tratamento do texto\n",
    "\n",
    "Remoção de emojis, mentions e hashtags para comparação de resultados ao final.\n",
    "\n",
    "Créditos ao time de dados do Estadão, que disponibilizou esse código utilizado em uma matéria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojistring = '''😀 😁 😂 🤣 😃 😄 😅 😆 😉 😊 😋 😎 😍 😘 😗 😙 😚 ☺️ 🙂 🤗 🤩 🤔 🤨 😐 😑 😶 🙄 😏 😣 😥 😮 🤐 😯 😪 😫 😴 😌 😛 😜 😝 🤤 😒 😓 😔 😕 🙃 🤑 😲 ☹️ 🙁 😖 😞 😟 😤 😢 😭 😦 😧 😨 😩 🤯 😬 😰 😱 😳 🤪 😵 😡 😠 🤬 😷 🤒 🤕 🤢 🤮 🤧 😇 🤠 🤡 🤥 🤫 🤭 🧐 🤓 😈 👿 👹 👺 💀 👻 👽 🤖 💩 😺 😸 😹 😻 😼 😽 🙀 😿 😾\n",
    "👶 👦 👧 👨 👩 👴 👵 👨‍⚕️ 👩‍⚕️ 👨‍🎓 👩‍🎓 👨‍⚖️ 👩‍⚖️ 👨‍🌾 👩‍🌾 👨‍🍳 👩‍🍳 👨‍🔧 👩‍🔧 👨‍🏭 👩‍🏭 👨‍💼 👩‍💼 👨‍🔬 👩‍🔬 👨‍💻 👩‍💻 👨‍🎤 👩‍🎤 👨‍🎨 👩‍🎨 👨‍✈️ 👩‍✈️ 👨‍🚀 👩‍🚀 👨‍🚒 👩‍🚒 👮 👮‍♂️ 👮‍♀️ 🕵 🕵️‍♂️ 🕵️‍♀️ 💂 💂‍♂️ 💂‍♀️ 👷 👷‍♂️ 👷‍♀️ 🤴 👸 👳 👳‍♂️ 👳‍♀️ 👲 🧕 🧔 👱 👱‍♂️ 👱‍♀️ 🤵 👰 🤰 🤱 👼 🎅 🤶 🧙‍♀️ 🧙‍♂️ 🧚‍♀️ 🧚‍♂️ 🧛‍♀️ 🧛‍♂️ 🧜‍♀️ 🧜‍♂️ 🧝‍♀️ 🧝‍♂️ 🧞‍♀️ 🧞‍♂️ 🧟‍♀️ 🧟‍♂️ 🙍 🙍‍♂️ 🙍‍♀️ 🙎 🙎‍♂️ 🙎‍♀️ 🙅 🙅‍♂️ 🙅‍♀️ 🙆 🙆‍♂️ 🙆‍♀️ 💁 💁‍♂️ 💁‍♀️ 🙋 🙋‍♂️ 🙋‍♀️ 🙇 🙇‍♂️ 🙇‍♀️ 🤦 🤦‍♂️ 🤦‍♀️ 🤷 🤷‍♂️ 🤷‍♀️ 💆 💆‍♂️ 💆‍♀️ 💇 💇‍♂️ 💇‍♀️ 🚶 🚶‍♂️ 🚶‍♀️ 🏃 🏃‍♂️ 🏃‍♀️ 💃 🕺 👯 👯‍♂️ 👯‍♀️ 🧖‍♀️ 🧖‍♂️ 🕴 🗣 👤 👥 👫 👬 👭 💏 👨‍❤️‍💋‍👨 👩‍❤️‍💋‍👩 💑 👨‍❤️‍👨 👩‍❤️‍👩 👪 👨‍👩‍👦 👨‍👩‍👧 👨‍👩‍👧‍👦 👨‍👩‍👦‍👦 👨‍👩‍👧‍👧 👨‍👨‍👦 👨‍👨‍👧 👨‍👨‍👧‍👦 👨‍👨‍👦‍👦 👨‍👨‍👧‍👧 👩‍👩‍👦 👩‍👩‍👧 👩‍👩‍👧‍👦 👩‍👩‍👦‍👦 👩‍👩‍👧‍👧 👨‍👦 👨‍👦‍👦 👨‍👧 👨‍👧‍👦 👨‍👧‍👧 👩‍👦 👩‍👦‍👦 👩‍👧 👩‍👧‍👦 👩‍👧‍👧 🤳 💪 👈 👉 ☝️ 👆 🖕 👇 ✌️ 🤞 🖖 🤘 🖐 ✋ 👌 👍 👎 ✊ 👊 🤛 🤜 🤚 👋 🤟 ✍️ 👏 👐 🙌 🤲 🙏 🤝 💅 👂 👃 👣 👀 👁 🧠 👅 👄 💋\n",
    "\n",
    "👓 🕶 👔 👕 👖 🧣 🧤 🧥 🧦 👗 👘 👙 👚 👛 👜 👝 🎒 👞 👟 👠 👡 👢 👑 👒 🎩 🎓 🧢 ⛑ 💄 💍 🌂 💼\n",
    "👐🏻 🙌🏻 👏🏻 🙏🏻 👍🏻 👎🏻 👊🏻 ✊🏻 🤛🏻 🤜🏻 🤞🏻 ✌🏻 🤘🏻 👌🏻 👈🏻 👉🏻 👆🏻 👇🏻 ☝🏻 ✋🏻 🤚🏻 🖐🏻 🖖🏻 👋🏻 🤙🏻 💪🏻 🖕🏻 ✍🏻 🤳🏻 💅🏻 👂🏻 👃🏻 👶🏻 👦🏻 👧🏻 👨🏻 👩🏻 👱🏻‍♀️ 👱🏻 👴🏻 👵🏻 👲🏻 👳🏻‍♀️ 👳🏻 👮🏻‍♀️ 👮🏻 👷🏻‍♀️ 👷🏻 💂🏻‍♀️ 💂🏻 🕵🏻‍♀️ 🕵🏻 👩🏻‍⚕️ 👨🏻‍⚕️ 👩🏻‍🌾 👨🏻‍🌾 👩🏻‍🍳 👨🏻‍🍳 👩🏻‍🎓 👨🏻‍🎓 👩🏻‍🎤 👨🏻‍🎤 👩🏻‍🏫 👨🏻‍🏫 👩🏻‍🏭 👨🏻‍🏭 👩🏻‍💻 👨🏻‍💻 👩🏻‍💼 👨🏻‍💼 👩🏻‍🔧 👨🏻‍🔧 👩🏻‍🔬 👨🏻‍🔬 👩🏻‍🎨 👨🏻‍🎨 👩🏻‍🚒 👨🏻‍🚒 👩🏻‍✈️ 👨🏻‍✈️ 👩🏻‍🚀 👨🏻‍🚀 👩🏻‍⚖️ 👨🏻‍⚖️ 🤶🏻 🎅🏻 👸🏻 🤴🏻 👰🏻 🤵🏻 👼🏻 🤰🏻 🙇🏻‍♀️ 🙇🏻 💁🏻 💁🏻‍♂️ 🙅🏻 🙅🏻‍♂️ 🙆🏻 🙆🏻‍♂️ 🙋🏻 🙋🏻‍♂️ 🤦🏻‍♀️ 🤦🏻‍♂️ 🤷🏻‍♀️ 🤷🏻‍♂️ 🙎🏻 🙎🏻‍♂️ 🙍🏻 🙍🏻‍♂️ 💇🏻 💇🏻‍♂️ 💆🏻 💆🏻‍♂️ 🕴🏻 💃🏻 🕺🏻 🚶🏻‍♀️ 🚶🏻 🏃🏻‍♀️ 🏃🏻 🏋🏻‍♀️ 🏋🏻 🤸🏻‍♀️ 🤸🏻‍♂️ ⛹🏻‍♀️ ⛹🏻 🤾🏻‍♀️ 🤾🏻‍♂️ 🏌🏻‍♀️ 🏌🏻 🏄🏻‍♀️ 🏄🏻 🏊🏻‍♀️ 🏊🏻 🤽🏻‍♀️ 🤽🏻‍♂️ 🚣🏻‍♀️ 🚣🏻 🏇🏻 🚴🏻‍♀️ 🚴🏻 🚵🏻‍♀️ 🚵🏻 🤹🏻‍♀️ 🤹🏻‍♂️ 🛀🏻\n",
    "\n",
    "👐🏼 🙌🏼 👏🏼 🙏🏼 👍🏼 👎🏼 👊🏼 ✊🏼 🤛🏼 🤜🏼 🤞🏼 ✌🏼 🤘🏼 👌🏼 👈🏼 👉🏼 👆🏼 👇🏼 ☝🏼 ✋🏼 🤚🏼 🖐🏼 🖖🏼 👋🏼 🤙🏼 💪🏼 🖕🏼 ✍🏼 🤳🏼 💅🏼 👂🏼 👃🏼 👶🏼 👦🏼 👧🏼 👨🏼 👩🏼 👱🏼‍♀️ 👱🏼 👴🏼 👵🏼 👲🏼 👳🏼‍♀️ 👳🏼 👮🏼‍♀️ 👮🏼 👷🏼‍♀️ 👷🏼 💂🏼‍♀️ 💂🏼 🕵🏼‍♀️ 🕵🏼 👩🏼‍⚕️ 👨🏼‍⚕️ 👩🏼‍🌾 👨🏼‍🌾 👩🏼‍🍳 👨🏼‍🍳 👩🏼‍🎓 👨🏼‍🎓 👩🏼‍🎤 👨🏼‍🎤 👩🏼‍🏫 👨🏼‍🏫 👩🏼‍🏭 👨🏼‍🏭 👩🏼‍💻 👨🏼‍💻 👩🏼‍💼 👨🏼‍💼 👩🏼‍🔧 👨🏼‍🔧 👩🏼‍🔬 👨🏼‍🔬 👩🏼‍🎨 👨🏼‍🎨 👩🏼‍🚒 👨🏼‍🚒 👩🏼‍✈️ 👨🏼‍✈️ 👩🏼‍🚀 👨🏼‍🚀 👩🏼‍⚖️ 👨🏼‍⚖️ 🤶🏼 🎅🏼 👸🏼 🤴🏼 👰🏼 🤵🏼 👼🏼 🤰🏼 🙇🏼‍♀️ 🙇🏼 💁🏼 💁🏼‍♂️ 🙅🏼 🙅🏼‍♂️ 🙆🏼 🙆🏼‍♂️ 🙋🏼 🙋🏼‍♂️ 🤦🏼‍♀️ 🤦🏼‍♂️ 🤷🏼‍♀️ 🤷🏼‍♂️ 🙎🏼 🙎🏼‍♂️ 🙍🏼 🙍🏼‍♂️ 💇🏼 💇🏼‍♂️ 💆🏼 💆🏼‍♂️ 🕴🏼 💃🏼 🕺🏼 🚶🏼‍♀️ 🚶🏼 🏃🏼‍♀️ 🏃🏼 🏋🏼‍♀️ 🏋🏼 🤸🏼‍♀️ 🤸🏼‍♂️ ⛹🏼‍♀️ ⛹🏼 🤾🏼‍♀️ 🤾🏼‍♂️ 🏌🏼‍♀️ 🏌🏼 🏄🏼‍♀️ 🏄🏼 🏊🏼‍♀️ 🏊🏼 🤽🏼‍♀️ 🤽🏼‍♂️ 🚣🏼‍♀️ 🚣🏼 🏇🏼 🚴🏼‍♀️ 🚴🏼 🚵🏼‍♀️ 🚵🏻 🤹🏼‍♀️ 🤹🏼‍♂️ 🛀🏼\n",
    "\n",
    "👐🏽 🙌🏽 👏🏽 🙏🏽 👍🏽 👎🏽 👊🏽 ✊🏽 🤛🏽 🤜🏽 🤞🏽 ✌🏽 🤘🏽 👌🏽 👈🏽 👉🏽 👆🏽 👇🏽 ☝🏽 ✋🏽 🤚🏽 🖐🏽 🖖🏽 👋🏽 🤙🏽 💪🏽 🖕🏽 ✍🏽 🤳🏽 💅🏽 👂🏽 👃🏽 👶🏽 👦🏽 👧🏽 👨🏽 👩🏽 👱🏽‍♀️ 👱🏽 👴🏽 👵🏽 👲🏽 👳🏽‍♀️ 👳🏽 👮🏽‍♀️ 👮🏽 👷🏽‍♀️ 👷🏽 💂🏽‍♀️ 💂🏽 🕵🏽‍♀️ 🕵🏽 👩🏽‍⚕️ 👨🏽‍⚕️ 👩🏽‍🌾 👨🏽‍🌾 👩🏽‍🍳 👨🏽‍🍳 👩🏽‍🎓 👨🏽‍🎓 👩🏽‍🎤 👨🏽‍🎤 👩🏽‍🏫 👨🏽‍🏫 👩🏽‍🏭 👨🏽‍🏭 👩🏽‍💻 👨🏽‍💻 👩🏽‍💼 👨🏽‍💼 👩🏽‍🔧 👨🏽‍🔧 👩🏽‍🔬 👨🏽‍🔬 👩🏽‍🎨 👨🏽‍🎨 👩🏽‍🚒 👨🏽‍🚒 👩🏽‍✈️ 👨🏽‍✈️ 👩🏽‍🚀 👨🏽‍🚀 👩🏽‍⚖️ 👨🏽‍⚖️ 🤶🏽 🎅🏽 👸🏽 🤴🏽 👰🏽 🤵🏽 👼🏽 🤰🏽 🙇🏽‍♀️ 🙇🏽 💁🏽 💁🏽‍♂️ 🙅🏽 🙅🏽‍♂️ 🙆🏽 🙆🏽‍♂️ 🙋🏽 🙋🏽‍♂️ 🤦🏽‍♀️ 🤦🏽‍♂️ 🤷🏽‍♀️ 🤷🏽‍♂️ 🙎🏽 🙎🏽‍♂️ 🙍🏽 🙍🏽‍♂️ 💇🏽 💇🏽‍♂️ 💆🏽 💆🏽‍♂️ 🕴🏼 💃🏽 🕺🏽 🚶🏽‍♀️ 🚶🏽 🏃🏽‍♀️ 🏃🏽 🏋🏽‍♀️ 🏋🏽 🤸🏽‍♀️ 🤸🏽‍♂️ ⛹🏽‍♀️ ⛹🏽 🤾🏽‍♀️ 🤾🏽‍♂️ 🏌🏽‍♀️ 🏌🏽 🏄🏽‍♀️ 🏄🏽 🏊🏽‍♀️ 🏊🏽 🤽🏽‍♀️ 🤽🏽‍♂️ 🚣🏽‍♀️ 🚣🏽 🏇🏽 🚴🏽‍♀️ 🚴🏽 🚵🏽‍♀️ 🚵🏽 🤹🏽‍♀️ 🤹🏽‍♂️ 🛀🏽\n",
    "\n",
    "👐🏾 🙌🏾 👏🏾 🙏🏾 👍🏾 👎🏾 👊🏾 ✊🏾 🤛🏾 🤜🏾 🤞🏾 ✌🏾 🤘🏾 👌🏾 👈🏾 👉🏾 👆🏾 👇🏾 ☝🏾 ✋🏾 🤚🏾 🖐🏾 🖖🏾 👋🏾 🤙🏾 💪🏾 🖕🏾 ✍🏾 🤳🏾 💅🏾 👂🏾 👃🏾 👶🏾 👦🏾 👧🏾 👨🏾 👩🏾 👱🏾‍♀️ 👱🏾 👴🏾 👵🏾 👲🏾 👳🏾‍♀️ 👳🏾 👮🏾‍♀️ 👮🏾 👷🏾‍♀️ 👷🏾 💂🏾‍♀️ 💂🏾 🕵🏾‍♀️ 🕵🏾 👩🏾‍⚕️ 👨🏾‍⚕️ 👩🏾‍🌾 👨🏾‍🌾 👩🏾‍🍳 👨🏾‍🍳 👩🏾‍🎓 👨🏾‍🎓 👩🏾‍🎤 👨🏾‍🎤 👩🏾‍🏫 👨🏾‍🏫 👩🏾‍🏭 👨🏾‍🏭 👩🏾‍💻 👨🏾‍💻 👩🏾‍💼 👨🏾‍💼 👩🏾‍🔧 👨🏾‍🔧 👩🏾‍🔬 👨🏾‍🔬 👩🏾‍🎨 👨🏾‍🎨 👩🏾‍🚒 👨🏾‍🚒 👩🏾‍✈️ 👨🏾‍✈️ 👩🏾‍🚀 👨🏾‍🚀 👩🏾‍⚖️ 👨🏾‍⚖️ 🤶🏾 🎅🏾 👸🏾 🤴🏾 👰🏾 🤵🏾 👼🏾 🤰🏾 🙇🏾‍♀️ 🙇🏾 💁🏾 💁🏾‍♂️ 🙅🏾 🙅🏾‍♂️ 🙆🏾 🙆🏾‍♂️ 🙋🏾 🙋🏾‍♂️ 🤦🏾‍♀️ 🤦🏾‍♂️ 🤷🏾‍♀️ 🤷🏾‍♂️ 🙎🏾 🙎🏾‍♂️ 🙍🏾 🙍🏾‍♂️ 💇🏾 💇🏾‍♂️ 💆🏾 💆🏾‍♂️ 🕴🏾 💃🏾 🕺🏾 🚶🏾‍♀️ 🚶🏾 🏃🏾‍♀️ 🏃🏾 🏋🏾‍♀️ 🏋🏾 🤸🏾‍♀️ 🤸🏾‍♂️ ⛹🏾‍♀️ ⛹🏾 🤾🏾‍♀️ 🤾🏾‍♂️ 🏌🏾‍♀️ 🏌🏾 🏄🏾‍♀️ 🏄🏾 🏊🏾‍♀️ 🏊🏾 🤽🏾‍♀️ 🤽🏾‍♂️ 🚣🏾‍♀️ 🚣🏾 🏇🏾 🚴🏾‍♀️ 🚴🏾 🚵🏾‍♀️ 🚵🏾 🤹🏾‍♀️ 🤹🏾‍♂️ 🛀🏾\n",
    "\n",
    "👐🏿 🙌🏿 👏🏿 🙏🏿 👍🏿 👎🏿 👊🏿 ✊🏿 🤛🏿 🤜🏿 🤞🏿 ✌🏿 🤘🏿 👌🏿 👈🏿 👉🏿 👆🏿 👇🏿 ☝🏿 ✋🏿 🤚🏿 🖐🏿 🖖🏿 👋🏿 🤙🏿 💪🏿 🖕🏿 ✍🏿 🤳🏿 💅🏿 👂🏿 👃🏿 👶🏿 👦🏿 👧🏿 👨🏿 👩🏿 👱🏿‍♀️ 👱🏿 👴🏿 👵🏿 👲🏿 👳🏿‍♀️ 👳🏿 👮🏿‍♀️ 👮🏿 👷🏿‍♀️ 👷🏿 💂🏿‍♀️ 💂🏿 🕵🏿‍♀️ 🕵🏿 👩🏿‍⚕️ 👨🏿‍⚕️ 👩🏿‍🌾 👨🏿‍🌾 👩🏿‍🍳 👨🏿‍🍳 👩🏿‍🎓 👨🏿‍🎓 👩🏿‍🎤 👨🏿‍🎤 👩🏿‍🏫 👨🏿‍🏫 👩🏿‍🏭 👨🏿‍🏭 👩🏿‍💻 👨🏿‍💻 👩🏿‍💼 👨🏿‍💼 👩🏿‍🔧 👨🏿‍🔧 👩🏿‍🔬 👨🏿‍🔬 👩🏿‍🎨 👨🏿‍🎨 👩🏿‍🚒 👨🏿‍🚒 👩🏿‍✈️ 👨🏿‍✈️ 👩🏿‍🚀 👨🏿‍🚀 👩🏿‍⚖️ 👨🏿‍⚖️ 🤶🏿 🎅🏿 👸🏿 🤴🏿 👰🏿 🤵🏿 👼🏿 🤰🏿 🙇🏿‍♀️ 🙇🏿 💁🏿 💁🏿‍♂️ 🙅🏿 🙅🏿‍♂️ 🙆🏿 🙆🏿‍♂️ 🙋🏿 🙋🏿‍♂️ 🤦🏿‍♀️ 🤦🏿‍♂️ 🤷🏿‍♀️ 🤷🏿‍♂️ 🙎🏿 🙎🏿‍♂️ 🙍🏿 🙍🏿‍♂️ 💇🏿 💇🏿‍♂️ 💆🏿 💆🏿‍♂️ 🕴🏿 💃🏿 🕺🏿 🚶🏿‍♀️ 🚶🏿 🏃🏿‍♀️ 🏃🏿 🏋🏿‍♀️ 🏋🏿 🤸🏿‍♀️ 🤸🏿‍♂️ ⛹🏿‍♀️ ⛹🏿 🤾🏿‍♀️ 🤾🏿‍♂️ 🏌🏿‍♀️ 🏌🏿 🏄🏿‍♀️ 🏄🏿 🏊🏿‍♀️ 🏊🏿 🤽🏿‍♀️ 🤽🏿‍♂️ 🚣🏿‍♀️ 🚣🏿 🏇🏿 🚴🏿‍♀️ 🚴🏿 🚵🏿‍♀️ 🚵🏿 🤹🏿‍♀️ 🤹🏿‍♂️ 🛀🏿\n",
    "\n",
    "🐶 🐱 🐭 🐹 🐰 🦊 🐻 🐼 🐨 🐯 🦁 🐮 🐷 🐽 🐸 🐵 🙊 🙉 🙊 🐒 🐔 🐧 🐦 🐤 🐣 🐥 🦆 🦅 🦉 🦇 🐺 🐗 🐴 🦄 🐝 🐛 🦋 🐌 🐚 🐞 🐜 🕷 🕸 🐢 🐍 🦎 🦂 🦀 🦑 🐙 🦐 🐠 🐟 🐡 🐬 🦈 🐳 🐋 🐊 🐆 🐅 🐃 🐂 🐄 🦌 🐪 🐫 🐘 🦏 🦍 🐎 🐖 🐐 🐏 🐑 🐕 🐩 🐈 🐓 🦃 🕊 🐇 🐁 🐀 🐿 🐾 🐉 🐲 🌵 🎄 🌲 🌳 🌴 🌱 🌿 ☘️ 🍀 🎍 🎋 🍃 🍂 🍁 🍄 🌾 💐 🌷 🌹 🥀 🌻 🌼 🌸 🌺 🌎 🌍 🌏 🌕 🌖 🌗 🌘 🌑 🌒 🌓 🌔 🌚 🌝 🌞 🌛 🌜 🌙 💫 ⭐️ 🌟 ✨ ⚡️ 🔥 💥 ☄️ ☀️ 🌤 ⛅️ 🌥 🌦 🌈 ☁️ 🌧 ⛈ 🌩 🌨 ☃️ ⛄️ ❄️ 🌬 💨 🌪 🌫 🌊 💧 💦 ☔️\n",
    "\n",
    "🍏 🍎 🍐 🍊 🍋 🍌 🍉 🍇 🍓 🍈 🍒 🍑 🍍 🥝 🥑 🍅 🍆 🥒 🥕 🌽 🌶 🥔 🍠 🌰 🥜 🍯 🥐 🍞 🥖 🧀 🥚 🍳 🥓 🥞 🍤 🍗 🍖 🍕 🌭 🍔 🍟 🥙 🌮 🌯 🥗 🥘 🍝 🍜 🍲 🍥 🍣 🍱 🍛 🍚 🍙 🍘 🍢 🍡 🍧 🍨 🍦 🍰 🎂 🍮 🍭 🍬 🍫 🍿 🍩 🍪 🥛 🍼 ☕️ 🍵 🍶 🍺 🍻 🥂 🍷 🥃 🍸 🍹 🍾 🥄 🍴 🍽\n",
    "\n",
    "⚽️ 🏀 🏈 ⚾️ 🎾 🏐 🏉 🎱 🏓 🏸 🥅 🏒 🏑 🏏 ⛳️ 🏹 🎣 🥊 🥋 ⛸ 🎿 ⛷ 🏂 🏋️‍♀️ 🏋️ 🤺 🤼‍♀️ 🤼‍♂️ 🤸‍♀️ 🤸‍♂️ ⛹️‍♀️ ⛹️ 🤾‍♀️ 🤾‍♂️ 🏌️‍♀️ 🏌️ 🏄‍♀️ 🏄 🏊‍♀️ 🏊 🤽‍♀️ 🤽‍♂️ 🚣‍♀️ 🚣 🏇 🚴‍♀️ 🚴 🚵‍♀️ 🚵 🎽 🏅 🎖 🥇 🥈 🥉 🏆 🏵 🎗 🎫 🎟 🎪 🤹‍♀️ 🤹‍♂️ 🎭 🎨 🎬 🎤 🎧 🎼 🎹 🥁 🎷 🎺 🎸 🎻 🎲 🎯 🎳 🎮 🎰\n",
    "\n",
    "🚗 🚕 🚙 🚌 🚎 🏎 🚓 🚑 🚒 🚐 🚚 🚛 🚜 🛴 🚲 🛵 🏍 🚨 🚔 🚍 🚘 🚖 🚡 🚠 🚟 🚃 🚋 🚞 🚝 🚄 🚅 🚈 🚂 🚆 🚇 🚊 🚉 🚁 🛩 ✈️ 🛫 🛬 🚀 🛰 💺 🛶 ⛵️ 🛥 🚤 🛳 ⛴ 🚢 ⚓️ 🚧 ⛽️ 🚏 🚦 🚥 🗺 🗿 🗽 ⛲️ 🗼 🏰 🏯 🏟 🎡 🎢 🎠 ⛱ 🏖 🏝 ⛰ 🏔 🗻 🌋 🏜 🏕 ⛺️ 🛤 🛣 🏗 🏭 🏠 🏡 🏘 🏚 🏢 🏬 🏣 🏤 🏥 🏦 🏨 🏪 🏫 🏩 💒 🏛 ⛪️ 🕌 🕍 🕋 ⛩ 🗾 🎑 🏞 🌅 🌄 🌠 🎇 🎆 🌇 🌆 🏙 🌃 🌌 🌉 🌁\n",
    "\n",
    "⌚️ 📱 📲 💻 ⌨️ 🖥 🖨 🖱 🖲 🕹 🗜 💽 💾 💿 📀 📼 📷 📸 📹 🎥 📽 🎞 📞 ☎️ 📟 📠 📺 📻 🎙 🎚 🎛 ⏱ ⏲ ⏰ 🕰 ⌛️ ⏳ 📡 🔋 🔌 💡 🔦 🕯 🗑 🛢 💸 💵 💴 💶 💷 💰 💳 💎 ⚖️ 🔧 🔨 ⚒ 🛠 ⛏ 🔩 ⚙️ ⛓ 🔫 💣 🔪 🗡 ⚔️ 🛡 🚬 ⚰️ ⚱️ 🏺 🔮 📿 💈 ⚗️ 🔭 🔬 🕳 💊 💉 🌡 🚽 🚰 🚿 🛁 🛀 🛎 🔑 🗝 🚪 🛋 🛏 🛌 🖼 🛍 🛒 🎁 🎈 🎏 🎀 🎊 🎉 🎎 🏮 🎐 ✉️ 📩 📨 📧 💌 📥 📤 📦 🏷 📪 📫 📬 📭 📮 📯 📜 📃 📄 📑 📊 📈 📉 🗒 🗓 📆 📅 📇 🗃 🗳 🗄 📋 📁 📂 🗂 🗞 📰 📓 📔 📒 📕 📗 📘 📙 📚 📖 🔖 🔗 📎 🖇 📐 📏 📌 📍 📌 🎌 🏳️ 🏴 🏁 🏳️‍🌈 ✂️ 🖊 🖋 ✒️ 🖌 🖍 📝 ✏️ 🔍 🔎 🔏 🔐 🔒 🔓\n",
    "\n",
    "❤️ 💛 💚 💙 💜 🖤 💔 ❣️ 💕 💞 💓 💗 💖 💘 💝 💟 ☮️ ✝️ ☪️ 🕉 ☸️ ✡️ 🔯 🕎 ☯️ ☦️ 🛐 ⛎ ♈️ ♉️ ♊️ ♋️ ♌️ ♍️ ♎️ ♏️ ♐️ ♑️ ♒️ ♓️ 🆔 ⚛️ 🉑 ☢️ ☣️ 📴 📳 🈶 🈚️ 🈸 🈺 🈷️ ✴️ 🆚 💮 🉐 ㊙️ ㊗️ 🈴 🈵 🈹 🈲 🅰️ 🅱️ 🆎 🆑 🅾️ 🆘 ❌ ⭕️ 🛑 ⛔️ 📛 🚫 💯 💢 ♨️ 🚷 🚯 🚳 🚱 🔞 📵 🚭 ❗️ ❕ ❓ ❔ ‼️ ⁉️ 🔅 🔆 〽️ ⚠️ 🚸 🔱 ⚜️ 🔰 ♻️ ✅ 🈯️ 💹 ❇️ ✳️ ❎ 🌐 💠 Ⓜ️ 🌀 💤 🏧 🚾 ♿️ 🅿️ 🈳 🈂️ 🛂 🛃 🛄 🛅 🚹 🚺 🚼 🚻 🚮 🎦 📶 🈁 🔣 ℹ️ 🔤 🔡 🔠 🆖 🆗 🆙 🆒 🆕 🆓 0️⃣ 1️⃣ 2️⃣ 3️⃣ 4️⃣ 5️⃣ 6️⃣ 7️⃣ 8️⃣ 9️⃣ 🔟 🔢 #️⃣ *️⃣ ▶️ ⏸ ⏯ ⏹ ⏺ ⏭ ⏮ ⏩ ⏪ ⏫ ⏬ ◀️ 🔼 🔽 ➡️ ⬅️ ⬆️ ⬇️ ↗️ ↘️ ↙️ ↖️ ↕️ ↔️ ↪️ ↩️ ⤴️ ⤵️ 🔀 🔁 🔂 🔄 🔃 🎵 🎶 ➕ ➖ ➗ ✖️ 💲 💱 ™️ ©️ ®️ 〰️ ➰ ➿ 🔚 🔙 🔛 🔝 ✔️ ☑️ 🔘 ⚪️ ⚫️ 🔴 🔵 🔺 🔻 🔸 🔹 🔶 🔷 🔳 🔲 ▪️ ▫️ ◾️ ◽️ ◼️ ◻️ ⬛️ ⬜️ 🔈 🔇 🔉 🔊 🔔 🔕 📣 📢 👁‍🗨 💬 💭 🗯 ♠️ ♣️ ♥️ ♦️ 🃏 🎴 🀄️ 🕐 🕑 🕒 🕓 🕔 🕕 🕖 🕗 🕘 🕙 🕚 🕛 🕜 🕝 🕞 🕟 🕠 🕡 🕢 🕣 🕤 🕥 🕦 🕧\n",
    "\n",
    "🏳️ 🏴 🏁 🚩 🏳️‍🌈 🇦🇫 🇦🇽 🇦🇱 🇩🇿 🇦🇸 🇦🇩 🇦🇴 🇦🇮 🇦🇶 🇦🇬 🇦🇷 🇦🇲 🇦🇼 🇦🇺 🇦🇹 🇦🇿 🇧🇸 🇧🇭 🇧🇩 🇧🇧 🇧🇾 🇧🇪 🇧🇿 🇧🇯 🇧🇲 🇧🇹 🇧🇴 🇧🇦 🇧🇼 🇧🇷 🇮🇴 🇻🇬 🇧🇳 🇧🇬 🇧🇫 🇧🇮 🇰🇭 🇨🇲 🇨🇦 🇮🇨 🇨🇻 🇧🇶 🇰🇾 🇨🇫 🇹🇩 🇨🇱 🇨🇳 🇨🇽 🇨🇨 🇨🇴 🇰🇲 🇨🇬 🇨🇩 🇨🇰 🇨🇷 🇨🇮 🇭🇷 🇨🇺 🇨🇼 🇨🇾 🇨🇿 🇩🇰 🇩🇯 🇩🇲 🇩🇴 🇪🇨 🇪🇬 🇸🇻 🇬🇶 🇪🇷 🇪🇪 🇪🇹 🇪🇺 🇫🇰 🇫🇴 🇫🇯 🇫🇮 🇫🇷 🇬🇫 🇵🇫 🇹🇫 🇬🇦 🇬🇲 🇬🇪 🇩🇪 🇬🇭 🇬🇮 🇬🇷 🇬🇱 🇬🇩 🇬🇵 🇬🇺 🇬🇹 🇬🇬 🇬🇳 🇬🇼 🇬🇾 🇭🇹 🇭🇳 🇭🇰 🇭🇺 🇮🇸 🇮🇳 🇮🇩 🇮🇷 🇮🇶 🇮🇪 🇮🇲 🇮🇱 🇮🇹 🇯🇲 🇯🇵 🎌 🇯🇪 🇯🇴 🇰🇿 🇰🇪 🇰🇮 🇽🇰 🇰🇼 🇰🇬 🇱🇦 🇱🇻 🇱🇧 🇱🇸 🇱🇷 🇱🇾 🇱🇮 🇱🇹 🇱🇺 🇲🇴 🇲🇰 🇲🇬 🇲🇼 🇲🇾 🇲🇻 🇲🇱 🇲🇹 🇲🇭 🇲🇶 🇲🇷 🇲🇺 🇾🇹 🇲🇽 🇫🇲 🇲🇩 🇲🇨 🇲🇳 🇲🇪 🇲🇸 🇲🇦 🇲🇿 🇲🇲 🇳🇦 🇳🇷 🇳🇵 🇳🇱 🇳🇨 🇳🇿 🇳🇮 🇳🇪 🇳🇬 🇳🇺 🇳🇫 🇰🇵 🇲🇵 🇳🇴 🇴🇲 🇵🇰 🇵🇼 🇵🇸 🇵🇦 🇵🇬 🇵🇾 🇵🇪 🇵🇭 🇵🇳 🇵🇱 🇵🇹 🇵🇷 🇶🇦 🇷🇪 🇷🇴 🇷🇺 🇷🇼 🇼🇸 🇸🇲 🇸🇦 🇸🇳 🇷🇸 🇸🇨 🇸🇱 🇸🇬 🇸🇽 🇸🇰 🇸🇮 🇬🇸 🇸🇧 🇸🇴 🇿🇦 🇰🇷 🇸🇸 🇪🇸 🇱🇰 🇧🇱 🇸🇭 🇰🇳 🇱🇨 🇵🇲 🇻🇨 🇸🇩 🇸🇷 🇸🇿 🇸🇪 🇨🇭 🇸🇾 🇹🇼 🇹🇯 🇹🇿 🇹🇭 🇹🇱 🇹🇬 🇹🇰 🇹🇴 🇹🇹 🇹🇳 🇹🇷 🇹🇲 🇹🇨 🇹🇻 🇻🇮 🇺🇬 🇺🇦 🇦🇪 🇬🇧 🏴󠁧󠁢󠁥󠁮󠁧󠁿 🏴󠁧󠁢󠁳󠁣󠁴󠁿 🏴󠁧󠁢󠁷󠁬󠁳󠁿 🇺🇸 🇺🇾 🇺🇿 🇻🇺 🇻🇦 🇻🇪 🇻🇳 🇼🇫 🇪🇭 🇾🇪 🇿🇲 🇿🇼\n",
    "'''\n",
    "\n",
    "\n",
    "def clean(row, string_with_chars_to_remove):\n",
    "    \n",
    "    # Cria uma lista de palavras\n",
    "    words = row.tweet.split()\n",
    "    \n",
    "    # Remove hashtags, mentions e links\n",
    "    words = [word for word in words if ( (word[0] not in ['#', '@']) \n",
    "             and (word.startswith('http') is False)\n",
    "             and (word.startswith('www.') is False)\n",
    "             and (word.startswith('t.co') is False)\n",
    "             and (word.startswith('bit.ly') is False)\n",
    "             and (word.startswith('goo.gl') is False)\n",
    "             and (word.startswith('migre.me') is False) )\n",
    "            ]\n",
    "\n",
    "    # Remove qualquer termo que não contenha nenhum caractere do alfabeto, incluindo acentos\n",
    "    words = [word for word in words if any(letter.isalpha() for letter in word)]\n",
    "    \n",
    "    # Remove pontuação do começo e final das palavras\n",
    "    for i in range(len(words)):\n",
    "        words[i] = words[i].strip((string.punctuation + emojistring))\n",
    "    \n",
    "    # Remove emojis do meio de palavras\n",
    "    # https://gist.github.com/Alex-Just/e86110836f3f93fe7932290526529cd1\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    for i in range(len(words)):\n",
    "        words[i] = RE_EMOJI.sub(r'', words[i])\n",
    "        \n",
    "    text = ' '.join(words)\n",
    "     \n",
    "    return pd.Series({'tweet':text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_classificados.csv')\n",
    "\n",
    "data = []\n",
    "clean_data = []\n",
    "data_labels = []\n",
    "\n",
    "df['clean_tweet'] = df.apply(clean, args=(emojistring,), axis=1)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    data.append(row['tweet'])\n",
    "    clean_data.append(row['clean_tweet'])\n",
    "    if row['review'] == 1:\n",
    "        data_labels.append('pos')\n",
    "    else:\n",
    "        data_labels.append('neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uma análise da quantidade de tweets em cada uma das classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1012"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Positivos\n",
    "len(df[df['review'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Negativos\n",
    "len(df[df['review'] == -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Neutros\n",
    "len(df[df['review'] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação dos parâmetros e classificadores\n",
    "\n",
    "Remover stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vc',\n",
       " 'de',\n",
       " 'a',\n",
       " 'o',\n",
       " 'que',\n",
       " 'e',\n",
       " 'do',\n",
       " 'da',\n",
       " 'em',\n",
       " 'um',\n",
       " 'para',\n",
       " 'é',\n",
       " 'com',\n",
       " 'uma',\n",
       " 'os',\n",
       " 'no',\n",
       " 'se',\n",
       " 'na',\n",
       " 'por',\n",
       " 'mais',\n",
       " 'as',\n",
       " 'dos',\n",
       " 'como',\n",
       " 'mas',\n",
       " 'foi',\n",
       " 'ao',\n",
       " 'ele',\n",
       " 'das',\n",
       " 'tem',\n",
       " 'à',\n",
       " 'seu',\n",
       " 'sua',\n",
       " 'ou',\n",
       " 'ser',\n",
       " 'quando',\n",
       " 'muito',\n",
       " 'há',\n",
       " 'nos',\n",
       " 'já',\n",
       " 'está',\n",
       " 'eu',\n",
       " 'também',\n",
       " 'só',\n",
       " 'pelo',\n",
       " 'pela',\n",
       " 'até',\n",
       " 'isso',\n",
       " 'ela',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'depois',\n",
       " 'sem',\n",
       " 'mesmo',\n",
       " 'aos',\n",
       " 'ter',\n",
       " 'seus',\n",
       " 'quem',\n",
       " 'nas',\n",
       " 'me',\n",
       " 'esse',\n",
       " 'eles',\n",
       " 'estão',\n",
       " 'você',\n",
       " 'tinha',\n",
       " 'foram',\n",
       " 'essa',\n",
       " 'num',\n",
       " 'nem',\n",
       " 'suas',\n",
       " 'meu',\n",
       " 'às',\n",
       " 'minha',\n",
       " 'têm',\n",
       " 'numa',\n",
       " 'pelos',\n",
       " 'elas',\n",
       " 'havia',\n",
       " 'seja',\n",
       " 'qual',\n",
       " 'será',\n",
       " 'nós',\n",
       " 'tenho',\n",
       " 'lhe',\n",
       " 'deles',\n",
       " 'essas',\n",
       " 'esses',\n",
       " 'pelas',\n",
       " 'este',\n",
       " 'fosse',\n",
       " 'dele',\n",
       " 'tu',\n",
       " 'te',\n",
       " 'vocês',\n",
       " 'vos',\n",
       " 'lhes',\n",
       " 'meus',\n",
       " 'minhas',\n",
       " 'teu',\n",
       " 'tua',\n",
       " 'teus',\n",
       " 'tuas',\n",
       " 'nosso',\n",
       " 'nossa',\n",
       " 'nossos',\n",
       " 'nossas',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'esta',\n",
       " 'estes',\n",
       " 'estas',\n",
       " 'aquele',\n",
       " 'aquela',\n",
       " 'aqueles',\n",
       " 'aquelas',\n",
       " 'isto',\n",
       " 'aquilo',\n",
       " 'estou',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estão',\n",
       " 'estive',\n",
       " 'esteve',\n",
       " 'estivemos',\n",
       " 'estiveram',\n",
       " 'estava',\n",
       " 'estávamos',\n",
       " 'estavam',\n",
       " 'estivera',\n",
       " 'estivéramos',\n",
       " 'esteja',\n",
       " 'estejamos',\n",
       " 'estejam',\n",
       " 'estivesse',\n",
       " 'estivéssemos',\n",
       " 'estivessem',\n",
       " 'estiver',\n",
       " 'estivermos',\n",
       " 'estiverem',\n",
       " 'hei',\n",
       " 'há',\n",
       " 'havemos',\n",
       " 'hão',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houveram',\n",
       " 'houvera',\n",
       " 'houvéramos',\n",
       " 'haja',\n",
       " 'hajamos',\n",
       " 'hajam',\n",
       " 'houvesse',\n",
       " 'houvéssemos',\n",
       " 'houvessem',\n",
       " 'houver',\n",
       " 'houvermos',\n",
       " 'houverem',\n",
       " 'houverei',\n",
       " 'houverá',\n",
       " 'houveremos',\n",
       " 'houverão',\n",
       " 'houveria',\n",
       " 'houveríamos',\n",
       " 'houveriam',\n",
       " 'sou',\n",
       " 'somos',\n",
       " 'são',\n",
       " 'era',\n",
       " 'éramos',\n",
       " 'eram',\n",
       " 'fui',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'foram',\n",
       " 'fora',\n",
       " 'fôramos',\n",
       " 'seja',\n",
       " 'sejamos',\n",
       " 'sejam',\n",
       " 'fosse',\n",
       " 'fôssemos',\n",
       " 'fossem',\n",
       " 'for',\n",
       " 'formos',\n",
       " 'forem',\n",
       " 'serei',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'serão',\n",
       " 'seria',\n",
       " 'seríamos',\n",
       " 'seriam',\n",
       " 'tenho',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 'tém',\n",
       " 'tinha',\n",
       " 'tínhamos',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'teve',\n",
       " 'tivemos',\n",
       " 'tiveram',\n",
       " 'tivera',\n",
       " 'tivéramos',\n",
       " 'tenha',\n",
       " 'tenhamos',\n",
       " 'tenham',\n",
       " 'tivesse',\n",
       " 'tivéssemos',\n",
       " 'tivessem',\n",
       " 'tiver',\n",
       " 'tivermos',\n",
       " 'tiverem',\n",
       " 'terei',\n",
       " 'terá',\n",
       " 'teremos',\n",
       " 'terão',\n",
       " 'teria',\n",
       " 'teríamos',\n",
       " 'teriam']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = [line.rstrip() for line in open('stopwords.txt')]\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params(data, data_labels, ngram):\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "        analyzer = 'word',\n",
    "        lowercase = False,\n",
    "        stop_words = stopwords,\n",
    "        ngram_range=(ngram, ngram)\n",
    "    )\n",
    "\n",
    "    features = vectorizer.fit_transform(\n",
    "        data\n",
    "    )\n",
    "\n",
    "    features_nd = features.toarray()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features_nd, \n",
    "            data_labels,\n",
    "            train_size=0.9,\n",
    "#             random_state = 42  #  Usando random state fixo para testar\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumb_logit_classifier(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    log_model = LogisticRegression()\n",
    "    log_model = log_model.fit(X=X_train, y=y_train)\n",
    "    y_pred = log_model.predict(X_test)\n",
    "\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classification(X_train, X_valid, y_train, y_valid):\n",
    "    \n",
    "    from sklearn import svm\n",
    "    svm = svm.SVC()\n",
    "    svm.fit(X_train, y_train)  \n",
    "    print(svm.score(X_valid, y_valid))\n",
    "    return svm.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alguns resultados de acurácia para a regressão logística\n",
    "\n",
    "Para unigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7450980392156863\n",
      "0.7156862745098039\n",
      "0.7647058823529411\n",
      "0.7450980392156863\n",
      "0.7352941176470589\n",
      "0.7254901960784313\n",
      "0.7941176470588235\n",
      "0.7745098039215687\n",
      "0.7254901960784313\n",
      "0.6568627450980392\n",
      "0.7647058823529411\n",
      "0.7745098039215687\n",
      "0.7254901960784313\n",
      "0.7647058823529411\n",
      "0.7254901960784313\n",
      "0.7843137254901961\n",
      "0.7843137254901961\n",
      "0.7549019607843137\n",
      "0.7450980392156863\n",
      "0.7647058823529411\n",
      "0.7843137254901961\n",
      "0.7647058823529411\n",
      "0.803921568627451\n",
      "0.6764705882352942\n",
      "0.7843137254901961\n",
      "0.803921568627451\n",
      "0.7254901960784313\n",
      "0.8137254901960784\n",
      "0.803921568627451\n",
      "0.7745098039215687\n",
      "0.7352941176470589\n",
      "0.7058823529411765\n",
      "0.7941176470588235\n",
      "0.7941176470588235\n",
      "0.7941176470588235\n",
      "0.7745098039215687\n",
      "0.7843137254901961\n",
      "0.8137254901960784\n",
      "0.7843137254901961\n",
      "0.6862745098039216\n",
      "0.7254901960784313\n",
      "0.7549019607843137\n",
      "0.7156862745098039\n",
      "0.7450980392156863\n",
      "0.7156862745098039\n",
      "0.7156862745098039\n",
      "0.7843137254901961\n",
      "0.7745098039215687\n",
      "0.8137254901960784\n",
      "0.7549019607843137\n",
      "0.7843137254901961\n",
      "0.7549019607843137\n",
      "0.8235294117647058\n",
      "0.7450980392156863\n",
      "0.7352941176470589\n",
      "0.803921568627451\n",
      "0.7450980392156863\n",
      "0.7254901960784313\n",
      "0.8431372549019608\n",
      "0.7647058823529411\n",
      "0.7843137254901961\n",
      "0.7450980392156863\n",
      "0.803921568627451\n",
      "0.803921568627451\n",
      "0.7745098039215687\n",
      "0.7254901960784313\n",
      "0.7549019607843137\n",
      "0.7254901960784313\n",
      "0.7843137254901961\n",
      "0.7745098039215687\n",
      "0.7941176470588235\n",
      "0.7745098039215687\n",
      "0.7254901960784313\n",
      "0.7941176470588235\n",
      "0.7450980392156863\n",
      "0.7843137254901961\n",
      "0.7843137254901961\n",
      "0.7352941176470589\n",
      "0.803921568627451\n",
      "0.7254901960784313\n",
      "0.7745098039215687\n",
      "0.7549019607843137\n",
      "0.7352941176470589\n",
      "0.7647058823529411\n",
      "0.7745098039215687\n",
      "0.7745098039215687\n",
      "0.7941176470588235\n",
      "0.7647058823529411\n",
      "0.7156862745098039\n",
      "0.8333333333333334\n",
      "0.7843137254901961\n",
      "0.7549019607843137\n",
      "0.8235294117647058\n",
      "0.7647058823529411\n",
      "0.696078431372549\n",
      "0.7352941176470589\n",
      "0.7745098039215687\n",
      "0.7843137254901961\n",
      "0.803921568627451\n",
      "0.6470588235294118\n"
     ]
    }
   ],
   "source": [
    "values_logint_uni = []\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = create_params(data, data_labels, 1)\n",
    "    values_logint_uni.append(dumb_logit_classifier(X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7619607843137255"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(values_logint_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para bigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7843137254901961\n",
      "0.7352941176470589\n",
      "0.803921568627451\n",
      "0.8137254901960784\n",
      "0.8333333333333334\n",
      "0.7647058823529411\n",
      "0.7254901960784313\n",
      "0.7745098039215687\n",
      "0.7254901960784313\n",
      "0.7156862745098039\n",
      "0.8235294117647058\n",
      "0.7450980392156863\n",
      "0.7745098039215687\n",
      "0.803921568627451\n",
      "0.7647058823529411\n",
      "0.7549019607843137\n",
      "0.7156862745098039\n",
      "0.803921568627451\n",
      "0.7450980392156863\n",
      "0.7450980392156863\n",
      "0.8529411764705882\n",
      "0.8529411764705882\n",
      "0.8235294117647058\n",
      "0.7647058823529411\n",
      "0.7647058823529411\n",
      "0.7941176470588235\n",
      "0.8431372549019608\n",
      "0.7254901960784313\n",
      "0.8235294117647058\n",
      "0.7647058823529411\n",
      "0.7450980392156863\n",
      "0.7450980392156863\n",
      "0.7156862745098039\n",
      "0.7058823529411765\n",
      "0.7843137254901961\n",
      "0.7843137254901961\n",
      "0.6568627450980392\n",
      "0.7352941176470589\n",
      "0.7843137254901961\n",
      "0.7745098039215687\n",
      "0.7549019607843137\n",
      "0.7745098039215687\n",
      "0.8431372549019608\n",
      "0.8137254901960784\n",
      "0.7843137254901961\n",
      "0.803921568627451\n",
      "0.7941176470588235\n",
      "0.7156862745098039\n",
      "0.8137254901960784\n",
      "0.7549019607843137\n",
      "0.7941176470588235\n",
      "0.803921568627451\n",
      "0.7745098039215687\n",
      "0.7647058823529411\n",
      "0.7941176470588235\n",
      "0.8333333333333334\n",
      "0.7058823529411765\n",
      "0.7352941176470589\n",
      "0.7450980392156863\n",
      "0.7941176470588235\n",
      "0.7647058823529411\n",
      "0.696078431372549\n",
      "0.7843137254901961\n",
      "0.7745098039215687\n",
      "0.8725490196078431\n",
      "0.7549019607843137\n",
      "0.8333333333333334\n",
      "0.7352941176470589\n",
      "0.7450980392156863\n",
      "0.7450980392156863\n",
      "0.7843137254901961\n",
      "0.7647058823529411\n",
      "0.7058823529411765\n",
      "0.7549019607843137\n",
      "0.7647058823529411\n",
      "0.7941176470588235\n",
      "0.7745098039215687\n",
      "0.6862745098039216\n",
      "0.7549019607843137\n",
      "0.8137254901960784\n",
      "0.7745098039215687\n",
      "0.7450980392156863\n",
      "0.803921568627451\n",
      "0.7843137254901961\n",
      "0.7647058823529411\n",
      "0.7647058823529411\n",
      "0.696078431372549\n",
      "0.7941176470588235\n",
      "0.7647058823529411\n",
      "0.7745098039215687\n",
      "0.803921568627451\n",
      "0.6666666666666666\n",
      "0.7156862745098039\n",
      "0.803921568627451\n",
      "0.7647058823529411\n",
      "0.7745098039215687\n",
      "0.6862745098039216\n",
      "0.7843137254901961\n",
      "0.7254901960784313\n",
      "0.803921568627451\n"
     ]
    }
   ],
   "source": [
    "values_logit_bi = []\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = create_params(data, data_labels, 2)\n",
    "    values_logit_bi.append(dumb_logit_classifier(X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.768921568627451"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(values_logit_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alguns resultados de acurácia para SVM\n",
    "\n",
    "Primeiramente para unigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7549019607843137\n",
      "0.7549019607843137\n",
      "0.7549019607843137\n",
      "0.7647058823529411\n",
      "0.7647058823529411\n",
      "0.8137254901960784\n",
      "0.7647058823529411\n",
      "0.803921568627451\n",
      "0.696078431372549\n",
      "0.7745098039215687\n",
      "0.7941176470588235\n",
      "0.6862745098039216\n",
      "0.7941176470588235\n",
      "0.8235294117647058\n",
      "0.7843137254901961\n",
      "0.7156862745098039\n",
      "0.7352941176470589\n",
      "0.7745098039215687\n",
      "0.803921568627451\n",
      "0.803921568627451\n",
      "0.7843137254901961\n",
      "0.7745098039215687\n",
      "0.8137254901960784\n",
      "0.7941176470588235\n",
      "0.8725490196078431\n",
      "0.8137254901960784\n",
      "0.7254901960784313\n",
      "0.7745098039215687\n",
      "0.7647058823529411\n",
      "0.7745098039215687\n",
      "0.7647058823529411\n",
      "0.7941176470588235\n",
      "0.7843137254901961\n",
      "0.7254901960784313\n",
      "0.803921568627451\n",
      "0.7647058823529411\n",
      "0.8235294117647058\n",
      "0.7549019607843137\n",
      "0.8627450980392157\n",
      "0.696078431372549\n",
      "0.7254901960784313\n",
      "0.7352941176470589\n",
      "0.7647058823529411\n",
      "0.7549019607843137\n",
      "0.7647058823529411\n",
      "0.803921568627451\n",
      "0.7352941176470589\n",
      "0.8137254901960784\n",
      "0.7352941176470589\n",
      "0.8627450980392157\n",
      "0.803921568627451\n",
      "0.7254901960784313\n",
      "0.7843137254901961\n",
      "0.8431372549019608\n",
      "0.696078431372549\n",
      "0.7549019607843137\n",
      "0.8137254901960784\n",
      "0.7647058823529411\n",
      "0.8137254901960784\n",
      "0.7647058823529411\n",
      "0.8333333333333334\n",
      "0.7941176470588235\n",
      "0.7843137254901961\n",
      "0.7941176470588235\n",
      "0.7941176470588235\n",
      "0.803921568627451\n",
      "0.8235294117647058\n",
      "0.803921568627451\n",
      "0.7549019607843137\n",
      "0.7549019607843137\n",
      "0.7745098039215687\n",
      "0.8137254901960784\n",
      "0.7843137254901961\n",
      "0.7450980392156863\n",
      "0.7156862745098039\n",
      "0.8137254901960784\n",
      "0.7843137254901961\n",
      "0.7352941176470589\n",
      "0.803921568627451\n",
      "0.8235294117647058\n",
      "0.7549019607843137\n",
      "0.803921568627451\n",
      "0.7549019607843137\n",
      "0.7254901960784313\n",
      "0.7941176470588235\n",
      "0.7941176470588235\n",
      "0.7647058823529411\n",
      "0.803921568627451\n",
      "0.7745098039215687\n",
      "0.7549019607843137\n",
      "0.7549019607843137\n",
      "0.8333333333333334\n",
      "0.8431372549019608\n",
      "0.7549019607843137\n",
      "0.7156862745098039\n",
      "0.8529411764705882\n",
      "0.7254901960784313\n",
      "0.7647058823529411\n",
      "0.8333333333333334\n",
      "0.7156862745098039\n"
     ]
    }
   ],
   "source": [
    "values_svm_uni = []\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = create_params(data, data_labels, 1)\n",
    "    values_svm_uni.append(svm_classification(X_train, X_test, y_train, y_test))\n",
    "print(mean(values_svm_uni))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora para bigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n",
      "0.7549019607843137\n",
      "0.8627450980392157\n",
      "0.7745098039215687\n",
      "0.8235294117647058\n",
      "0.7745098039215687\n",
      "0.803921568627451\n",
      "0.8725490196078431\n",
      "0.8137254901960784\n",
      "0.7450980392156863\n",
      "0.8235294117647058\n",
      "0.7450980392156863\n",
      "0.7647058823529411\n",
      "0.8627450980392157\n",
      "0.7745098039215687\n",
      "0.7745098039215687\n",
      "0.696078431372549\n",
      "0.7450980392156863\n",
      "0.7549019607843137\n",
      "0.7843137254901961\n",
      "0.7941176470588235\n",
      "0.8333333333333334\n",
      "0.7647058823529411\n",
      "0.7843137254901961\n",
      "0.7549019607843137\n",
      "0.7745098039215687\n",
      "0.7549019607843137\n",
      "0.7058823529411765\n",
      "0.8627450980392157\n",
      "0.7156862745098039\n",
      "0.7156862745098039\n",
      "0.8431372549019608\n",
      "0.7843137254901961\n",
      "0.8431372549019608\n",
      "0.7450980392156863\n",
      "0.7745098039215687\n",
      "0.7941176470588235\n",
      "0.803921568627451\n",
      "0.7941176470588235\n",
      "0.8235294117647058\n",
      "0.6372549019607843\n",
      "0.8235294117647058\n",
      "0.8529411764705882\n",
      "0.7254901960784313\n",
      "0.7254901960784313\n",
      "0.8137254901960784\n",
      "0.7254901960784313\n",
      "0.8137254901960784\n",
      "0.8137254901960784\n",
      "0.7647058823529411\n",
      "0.7647058823529411\n",
      "0.7254901960784313\n",
      "0.7941176470588235\n",
      "0.7745098039215687\n",
      "0.7745098039215687\n",
      "0.803921568627451\n",
      "0.7745098039215687\n",
      "0.7549019607843137\n",
      "0.8431372549019608\n",
      "0.7254901960784313\n",
      "0.7254901960784313\n",
      "0.7254901960784313\n",
      "0.7647058823529411\n",
      "0.8137254901960784\n",
      "0.7549019607843137\n",
      "0.7745098039215687\n",
      "0.7450980392156863\n",
      "0.7450980392156863\n",
      "0.8235294117647058\n",
      "0.8235294117647058\n",
      "0.7352941176470589\n",
      "0.8529411764705882\n",
      "0.7254901960784313\n",
      "0.7745098039215687\n",
      "0.7647058823529411\n",
      "0.7254901960784313\n",
      "0.7254901960784313\n",
      "0.7647058823529411\n",
      "0.7450980392156863\n",
      "0.8137254901960784\n",
      "0.803921568627451\n",
      "0.7745098039215687\n",
      "0.7450980392156863\n",
      "0.7549019607843137\n",
      "0.7843137254901961\n",
      "0.7941176470588235\n",
      "0.7450980392156863\n",
      "0.7941176470588235\n",
      "0.7352941176470589\n",
      "0.7549019607843137\n",
      "0.7156862745098039\n",
      "0.7450980392156863\n",
      "0.8235294117647058\n",
      "0.7941176470588235\n",
      "0.7745098039215687\n",
      "0.7352941176470589\n",
      "0.7254901960784313\n",
      "0.7058823529411765\n",
      "0.7058823529411765\n",
      "0.7450980392156863\n"
     ]
    }
   ],
   "source": [
    "values_svm_bi = []\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = create_params(clean_data, data_labels, 2)\n",
    "    values_svm_bi.append(svm_classification(X_train, X_test, y_train, y_test))\n",
    "print(mean(values_svm_bi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Os resultados estão de acordo com a teoria: os bigramas perfomam melhor, assim como SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "\n",
    "-- http://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1012 points : 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9812252964426877"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "ngram = 2\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    "    ngram_range=(ngram, ngram)\n",
    ")\n",
    "\n",
    "features = vectorizer.fit_transform(\n",
    "    data\n",
    ")\n",
    "\n",
    "features_nd = features.toarray()\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(features_nd, data_labels).predict(features_nd)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (features_nd.shape[0],(data_labels != y_pred).sum()))\n",
    "\n",
    "gnb.score(features_nd, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1012 points : 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9812252964426877"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "ngram = 2\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    "    stop_words = stopwords,\n",
    "    ngram_range=(ngram, ngram)\n",
    ")\n",
    "\n",
    "features = vectorizer.fit_transform(\n",
    "    data\n",
    ")\n",
    "\n",
    "features_nd = features.toarray()\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(features_nd, data_labels).predict(features_nd)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (features_nd.shape[0],(data_labels != y_pred).sum()))\n",
    "\n",
    "gnb.score(features_nd, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1012 points : 29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9713438735177866"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram = 1\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    "    stop_words = stopwords,\n",
    "    ngram_range=(ngram, ngram)\n",
    ")\n",
    "\n",
    "features = vectorizer.fit_transform(\n",
    "    data\n",
    ")\n",
    "\n",
    "features_nd = features.toarray()\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(features_nd, data_labels).predict(features_nd)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (features_nd.shape[0],(data_labels != y_pred).sum()))\n",
    "\n",
    "gnb.score(features_nd, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1012 points : 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9723320158102767"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram = 1\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    "    ngram_range=(ngram, ngram)\n",
    ")\n",
    "\n",
    "features = vectorizer.fit_transform(\n",
    "    data\n",
    ")\n",
    "\n",
    "features_nd = features.toarray()\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(features_nd, data_labels).predict(features_nd)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (features_nd.shape[0],(data_labels != y_pred).sum()))\n",
    "\n",
    "gnb.score(features_nd, data_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Para evitar overfitting. Mais aqui: http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "\n",
    "SVC\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77832512, 0.77832512, 0.77722772, 0.77722772, 0.77722772])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "ngram = 2\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    "    ngram_range=(ngram, ngram)\n",
    ")\n",
    "\n",
    "features = vectorizer.fit_transform(\n",
    "    data\n",
    ")\n",
    "\n",
    "features_nd = features.toarray()\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=0.0001)\n",
    "scores = cross_val_score(clf, features_nd, data_labels, cv=5)\n",
    "scores \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77832512 0.77832512 0.77722772 0.77722772 0.77722772]\n",
      "Accuracy: 0.78 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='rbf', C=0.0001)\n",
    "scores = cross_val_score(clf, features_nd, data_labels, cv=5)\n",
    "print(scores) \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77832512 0.77832512 0.77722772 0.77722772 0.77722772]\n",
      "Accuracy: 0.78 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='rbf', C=1)\n",
    "scores = cross_val_score(clf, features_nd, data_labels, cv=5)\n",
    "print(scores) \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77832512 0.77832512 0.77722772 0.77722772 0.77722772]\n",
      "Accuracy: 0.78 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=0.0001)\n",
    "scores = cross_val_score(clf, features_nd, data_labels, cv=5)\n",
    "print(scores) \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79310345 0.77832512 0.76732673 0.78217822 0.78217822]\n",
      "Accuracy: 0.78 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, features_nd, data_labels, cv=5)\n",
    "print(scores) \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunando os hiperparâmetros\n",
    "\n",
    "Grid search:: \n",
    "\n",
    "-- http://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n",
    "-- https://medium.com/@aneesha/svm-parameter-tuning-in-scikit-learn-using-gridsearchcv-2413c02125a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

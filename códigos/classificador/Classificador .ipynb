{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ClassificaÃ§Ã£o de tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helfs/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FunÃ§Ãµes para tratamento do texto\n",
    "\n",
    "RemoÃ§Ã£o de emojis, mentions e hashtags para comparaÃ§Ã£o de resultados ao final.\n",
    "\n",
    "CrÃ©ditos ao time de dados do EstadÃ£o, que disponibilizou esse cÃ³digo utilizado em uma matÃ©ria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojistring = '''ğŸ˜€ ğŸ˜ ğŸ˜‚ ğŸ¤£ ğŸ˜ƒ ğŸ˜„ ğŸ˜… ğŸ˜† ğŸ˜‰ ğŸ˜Š ğŸ˜‹ ğŸ˜ ğŸ˜ ğŸ˜˜ ğŸ˜— ğŸ˜™ ğŸ˜š â˜ºï¸ ğŸ™‚ ğŸ¤— ğŸ¤© ğŸ¤” ğŸ¤¨ ğŸ˜ ğŸ˜‘ ğŸ˜¶ ğŸ™„ ğŸ˜ ğŸ˜£ ğŸ˜¥ ğŸ˜® ğŸ¤ ğŸ˜¯ ğŸ˜ª ğŸ˜« ğŸ˜´ ğŸ˜Œ ğŸ˜› ğŸ˜œ ğŸ˜ ğŸ¤¤ ğŸ˜’ ğŸ˜“ ğŸ˜” ğŸ˜• ğŸ™ƒ ğŸ¤‘ ğŸ˜² â˜¹ï¸ ğŸ™ ğŸ˜– ğŸ˜ ğŸ˜Ÿ ğŸ˜¤ ğŸ˜¢ ğŸ˜­ ğŸ˜¦ ğŸ˜§ ğŸ˜¨ ğŸ˜© ğŸ¤¯ ğŸ˜¬ ğŸ˜° ğŸ˜± ğŸ˜³ ğŸ¤ª ğŸ˜µ ğŸ˜¡ ğŸ˜  ğŸ¤¬ ğŸ˜· ğŸ¤’ ğŸ¤• ğŸ¤¢ ğŸ¤® ğŸ¤§ ğŸ˜‡ ğŸ¤  ğŸ¤¡ ğŸ¤¥ ğŸ¤« ğŸ¤­ ğŸ§ ğŸ¤“ ğŸ˜ˆ ğŸ‘¿ ğŸ‘¹ ğŸ‘º ğŸ’€ ğŸ‘» ğŸ‘½ ğŸ¤– ğŸ’© ğŸ˜º ğŸ˜¸ ğŸ˜¹ ğŸ˜» ğŸ˜¼ ğŸ˜½ ğŸ™€ ğŸ˜¿ ğŸ˜¾\n",
    "ğŸ‘¶ ğŸ‘¦ ğŸ‘§ ğŸ‘¨ ğŸ‘© ğŸ‘´ ğŸ‘µ ğŸ‘¨â€âš•ï¸ ğŸ‘©â€âš•ï¸ ğŸ‘¨â€ğŸ“ ğŸ‘©â€ğŸ“ ğŸ‘¨â€âš–ï¸ ğŸ‘©â€âš–ï¸ ğŸ‘¨â€ğŸŒ¾ ğŸ‘©â€ğŸŒ¾ ğŸ‘¨â€ğŸ³ ğŸ‘©â€ğŸ³ ğŸ‘¨â€ğŸ”§ ğŸ‘©â€ğŸ”§ ğŸ‘¨â€ğŸ­ ğŸ‘©â€ğŸ­ ğŸ‘¨â€ğŸ’¼ ğŸ‘©â€ğŸ’¼ ğŸ‘¨â€ğŸ”¬ ğŸ‘©â€ğŸ”¬ ğŸ‘¨â€ğŸ’» ğŸ‘©â€ğŸ’» ğŸ‘¨â€ğŸ¤ ğŸ‘©â€ğŸ¤ ğŸ‘¨â€ğŸ¨ ğŸ‘©â€ğŸ¨ ğŸ‘¨â€âœˆï¸ ğŸ‘©â€âœˆï¸ ğŸ‘¨â€ğŸš€ ğŸ‘©â€ğŸš€ ğŸ‘¨â€ğŸš’ ğŸ‘©â€ğŸš’ ğŸ‘® ğŸ‘®â€â™‚ï¸ ğŸ‘®â€â™€ï¸ ğŸ•µ ğŸ•µï¸â€â™‚ï¸ ğŸ•µï¸â€â™€ï¸ ğŸ’‚ ğŸ’‚â€â™‚ï¸ ğŸ’‚â€â™€ï¸ ğŸ‘· ğŸ‘·â€â™‚ï¸ ğŸ‘·â€â™€ï¸ ğŸ¤´ ğŸ‘¸ ğŸ‘³ ğŸ‘³â€â™‚ï¸ ğŸ‘³â€â™€ï¸ ğŸ‘² ğŸ§• ğŸ§” ğŸ‘± ğŸ‘±â€â™‚ï¸ ğŸ‘±â€â™€ï¸ ğŸ¤µ ğŸ‘° ğŸ¤° ğŸ¤± ğŸ‘¼ ğŸ… ğŸ¤¶ ğŸ§™â€â™€ï¸ ğŸ§™â€â™‚ï¸ ğŸ§šâ€â™€ï¸ ğŸ§šâ€â™‚ï¸ ğŸ§›â€â™€ï¸ ğŸ§›â€â™‚ï¸ ğŸ§œâ€â™€ï¸ ğŸ§œâ€â™‚ï¸ ğŸ§â€â™€ï¸ ğŸ§â€â™‚ï¸ ğŸ§â€â™€ï¸ ğŸ§â€â™‚ï¸ ğŸ§Ÿâ€â™€ï¸ ğŸ§Ÿâ€â™‚ï¸ ğŸ™ ğŸ™â€â™‚ï¸ ğŸ™â€â™€ï¸ ğŸ™ ğŸ™â€â™‚ï¸ ğŸ™â€â™€ï¸ ğŸ™… ğŸ™…â€â™‚ï¸ ğŸ™…â€â™€ï¸ ğŸ™† ğŸ™†â€â™‚ï¸ ğŸ™†â€â™€ï¸ ğŸ’ ğŸ’â€â™‚ï¸ ğŸ’â€â™€ï¸ ğŸ™‹ ğŸ™‹â€â™‚ï¸ ğŸ™‹â€â™€ï¸ ğŸ™‡ ğŸ™‡â€â™‚ï¸ ğŸ™‡â€â™€ï¸ ğŸ¤¦ ğŸ¤¦â€â™‚ï¸ ğŸ¤¦â€â™€ï¸ ğŸ¤· ğŸ¤·â€â™‚ï¸ ğŸ¤·â€â™€ï¸ ğŸ’† ğŸ’†â€â™‚ï¸ ğŸ’†â€â™€ï¸ ğŸ’‡ ğŸ’‡â€â™‚ï¸ ğŸ’‡â€â™€ï¸ ğŸš¶ ğŸš¶â€â™‚ï¸ ğŸš¶â€â™€ï¸ ğŸƒ ğŸƒâ€â™‚ï¸ ğŸƒâ€â™€ï¸ ğŸ’ƒ ğŸ•º ğŸ‘¯ ğŸ‘¯â€â™‚ï¸ ğŸ‘¯â€â™€ï¸ ğŸ§–â€â™€ï¸ ğŸ§–â€â™‚ï¸ ğŸ•´ ğŸ—£ ğŸ‘¤ ğŸ‘¥ ğŸ‘« ğŸ‘¬ ğŸ‘­ ğŸ’ ğŸ‘¨â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ ğŸ‘©â€â¤ï¸â€ğŸ’‹â€ğŸ‘© ğŸ’‘ ğŸ‘¨â€â¤ï¸â€ğŸ‘¨ ğŸ‘©â€â¤ï¸â€ğŸ‘© ğŸ‘ª ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘§ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§â€ğŸ‘§ ğŸ‘©â€ğŸ‘©â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘§ ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘§ ğŸ‘¨â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘§ ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘§â€ğŸ‘§ ğŸ‘©â€ğŸ‘¦ ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘©â€ğŸ‘§ ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘©â€ğŸ‘§â€ğŸ‘§ ğŸ¤³ ğŸ’ª ğŸ‘ˆ ğŸ‘‰ â˜ï¸ ğŸ‘† ğŸ–• ğŸ‘‡ âœŒï¸ ğŸ¤ ğŸ–– ğŸ¤˜ ğŸ– âœ‹ ğŸ‘Œ ğŸ‘ ğŸ‘ âœŠ ğŸ‘Š ğŸ¤› ğŸ¤œ ğŸ¤š ğŸ‘‹ ğŸ¤Ÿ âœï¸ ğŸ‘ ğŸ‘ ğŸ™Œ ğŸ¤² ğŸ™ ğŸ¤ ğŸ’… ğŸ‘‚ ğŸ‘ƒ ğŸ‘£ ğŸ‘€ ğŸ‘ ğŸ§  ğŸ‘… ğŸ‘„ ğŸ’‹\n",
    "\n",
    "ğŸ‘“ ğŸ•¶ ğŸ‘” ğŸ‘• ğŸ‘– ğŸ§£ ğŸ§¤ ğŸ§¥ ğŸ§¦ ğŸ‘— ğŸ‘˜ ğŸ‘™ ğŸ‘š ğŸ‘› ğŸ‘œ ğŸ‘ ğŸ’ ğŸ‘ ğŸ‘Ÿ ğŸ‘  ğŸ‘¡ ğŸ‘¢ ğŸ‘‘ ğŸ‘’ ğŸ© ğŸ“ ğŸ§¢ â›‘ ğŸ’„ ğŸ’ ğŸŒ‚ ğŸ’¼\n",
    "ğŸ‘ğŸ» ğŸ™ŒğŸ» ğŸ‘ğŸ» ğŸ™ğŸ» ğŸ‘ğŸ» ğŸ‘ğŸ» ğŸ‘ŠğŸ» âœŠğŸ» ğŸ¤›ğŸ» ğŸ¤œğŸ» ğŸ¤ğŸ» âœŒğŸ» ğŸ¤˜ğŸ» ğŸ‘ŒğŸ» ğŸ‘ˆğŸ» ğŸ‘‰ğŸ» ğŸ‘†ğŸ» ğŸ‘‡ğŸ» â˜ğŸ» âœ‹ğŸ» ğŸ¤šğŸ» ğŸ–ğŸ» ğŸ––ğŸ» ğŸ‘‹ğŸ» ğŸ¤™ğŸ» ğŸ’ªğŸ» ğŸ–•ğŸ» âœğŸ» ğŸ¤³ğŸ» ğŸ’…ğŸ» ğŸ‘‚ğŸ» ğŸ‘ƒğŸ» ğŸ‘¶ğŸ» ğŸ‘¦ğŸ» ğŸ‘§ğŸ» ğŸ‘¨ğŸ» ğŸ‘©ğŸ» ğŸ‘±ğŸ»â€â™€ï¸ ğŸ‘±ğŸ» ğŸ‘´ğŸ» ğŸ‘µğŸ» ğŸ‘²ğŸ» ğŸ‘³ğŸ»â€â™€ï¸ ğŸ‘³ğŸ» ğŸ‘®ğŸ»â€â™€ï¸ ğŸ‘®ğŸ» ğŸ‘·ğŸ»â€â™€ï¸ ğŸ‘·ğŸ» ğŸ’‚ğŸ»â€â™€ï¸ ğŸ’‚ğŸ» ğŸ•µğŸ»â€â™€ï¸ ğŸ•µğŸ» ğŸ‘©ğŸ»â€âš•ï¸ ğŸ‘¨ğŸ»â€âš•ï¸ ğŸ‘©ğŸ»â€ğŸŒ¾ ğŸ‘¨ğŸ»â€ğŸŒ¾ ğŸ‘©ğŸ»â€ğŸ³ ğŸ‘¨ğŸ»â€ğŸ³ ğŸ‘©ğŸ»â€ğŸ“ ğŸ‘¨ğŸ»â€ğŸ“ ğŸ‘©ğŸ»â€ğŸ¤ ğŸ‘¨ğŸ»â€ğŸ¤ ğŸ‘©ğŸ»â€ğŸ« ğŸ‘¨ğŸ»â€ğŸ« ğŸ‘©ğŸ»â€ğŸ­ ğŸ‘¨ğŸ»â€ğŸ­ ğŸ‘©ğŸ»â€ğŸ’» ğŸ‘¨ğŸ»â€ğŸ’» ğŸ‘©ğŸ»â€ğŸ’¼ ğŸ‘¨ğŸ»â€ğŸ’¼ ğŸ‘©ğŸ»â€ğŸ”§ ğŸ‘¨ğŸ»â€ğŸ”§ ğŸ‘©ğŸ»â€ğŸ”¬ ğŸ‘¨ğŸ»â€ğŸ”¬ ğŸ‘©ğŸ»â€ğŸ¨ ğŸ‘¨ğŸ»â€ğŸ¨ ğŸ‘©ğŸ»â€ğŸš’ ğŸ‘¨ğŸ»â€ğŸš’ ğŸ‘©ğŸ»â€âœˆï¸ ğŸ‘¨ğŸ»â€âœˆï¸ ğŸ‘©ğŸ»â€ğŸš€ ğŸ‘¨ğŸ»â€ğŸš€ ğŸ‘©ğŸ»â€âš–ï¸ ğŸ‘¨ğŸ»â€âš–ï¸ ğŸ¤¶ğŸ» ğŸ…ğŸ» ğŸ‘¸ğŸ» ğŸ¤´ğŸ» ğŸ‘°ğŸ» ğŸ¤µğŸ» ğŸ‘¼ğŸ» ğŸ¤°ğŸ» ğŸ™‡ğŸ»â€â™€ï¸ ğŸ™‡ğŸ» ğŸ’ğŸ» ğŸ’ğŸ»â€â™‚ï¸ ğŸ™…ğŸ» ğŸ™…ğŸ»â€â™‚ï¸ ğŸ™†ğŸ» ğŸ™†ğŸ»â€â™‚ï¸ ğŸ™‹ğŸ» ğŸ™‹ğŸ»â€â™‚ï¸ ğŸ¤¦ğŸ»â€â™€ï¸ ğŸ¤¦ğŸ»â€â™‚ï¸ ğŸ¤·ğŸ»â€â™€ï¸ ğŸ¤·ğŸ»â€â™‚ï¸ ğŸ™ğŸ» ğŸ™ğŸ»â€â™‚ï¸ ğŸ™ğŸ» ğŸ™ğŸ»â€â™‚ï¸ ğŸ’‡ğŸ» ğŸ’‡ğŸ»â€â™‚ï¸ ğŸ’†ğŸ» ğŸ’†ğŸ»â€â™‚ï¸ ğŸ•´ğŸ» ğŸ’ƒğŸ» ğŸ•ºğŸ» ğŸš¶ğŸ»â€â™€ï¸ ğŸš¶ğŸ» ğŸƒğŸ»â€â™€ï¸ ğŸƒğŸ» ğŸ‹ğŸ»â€â™€ï¸ ğŸ‹ğŸ» ğŸ¤¸ğŸ»â€â™€ï¸ ğŸ¤¸ğŸ»â€â™‚ï¸ â›¹ğŸ»â€â™€ï¸ â›¹ğŸ» ğŸ¤¾ğŸ»â€â™€ï¸ ğŸ¤¾ğŸ»â€â™‚ï¸ ğŸŒğŸ»â€â™€ï¸ ğŸŒğŸ» ğŸ„ğŸ»â€â™€ï¸ ğŸ„ğŸ» ğŸŠğŸ»â€â™€ï¸ ğŸŠğŸ» ğŸ¤½ğŸ»â€â™€ï¸ ğŸ¤½ğŸ»â€â™‚ï¸ ğŸš£ğŸ»â€â™€ï¸ ğŸš£ğŸ» ğŸ‡ğŸ» ğŸš´ğŸ»â€â™€ï¸ ğŸš´ğŸ» ğŸšµğŸ»â€â™€ï¸ ğŸšµğŸ» ğŸ¤¹ğŸ»â€â™€ï¸ ğŸ¤¹ğŸ»â€â™‚ï¸ ğŸ›€ğŸ»\n",
    "\n",
    "ğŸ‘ğŸ¼ ğŸ™ŒğŸ¼ ğŸ‘ğŸ¼ ğŸ™ğŸ¼ ğŸ‘ğŸ¼ ğŸ‘ğŸ¼ ğŸ‘ŠğŸ¼ âœŠğŸ¼ ğŸ¤›ğŸ¼ ğŸ¤œğŸ¼ ğŸ¤ğŸ¼ âœŒğŸ¼ ğŸ¤˜ğŸ¼ ğŸ‘ŒğŸ¼ ğŸ‘ˆğŸ¼ ğŸ‘‰ğŸ¼ ğŸ‘†ğŸ¼ ğŸ‘‡ğŸ¼ â˜ğŸ¼ âœ‹ğŸ¼ ğŸ¤šğŸ¼ ğŸ–ğŸ¼ ğŸ––ğŸ¼ ğŸ‘‹ğŸ¼ ğŸ¤™ğŸ¼ ğŸ’ªğŸ¼ ğŸ–•ğŸ¼ âœğŸ¼ ğŸ¤³ğŸ¼ ğŸ’…ğŸ¼ ğŸ‘‚ğŸ¼ ğŸ‘ƒğŸ¼ ğŸ‘¶ğŸ¼ ğŸ‘¦ğŸ¼ ğŸ‘§ğŸ¼ ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¼ ğŸ‘±ğŸ¼â€â™€ï¸ ğŸ‘±ğŸ¼ ğŸ‘´ğŸ¼ ğŸ‘µğŸ¼ ğŸ‘²ğŸ¼ ğŸ‘³ğŸ¼â€â™€ï¸ ğŸ‘³ğŸ¼ ğŸ‘®ğŸ¼â€â™€ï¸ ğŸ‘®ğŸ¼ ğŸ‘·ğŸ¼â€â™€ï¸ ğŸ‘·ğŸ¼ ğŸ’‚ğŸ¼â€â™€ï¸ ğŸ’‚ğŸ¼ ğŸ•µğŸ¼â€â™€ï¸ ğŸ•µğŸ¼ ğŸ‘©ğŸ¼â€âš•ï¸ ğŸ‘¨ğŸ¼â€âš•ï¸ ğŸ‘©ğŸ¼â€ğŸŒ¾ ğŸ‘¨ğŸ¼â€ğŸŒ¾ ğŸ‘©ğŸ¼â€ğŸ³ ğŸ‘¨ğŸ¼â€ğŸ³ ğŸ‘©ğŸ¼â€ğŸ“ ğŸ‘¨ğŸ¼â€ğŸ“ ğŸ‘©ğŸ¼â€ğŸ¤ ğŸ‘¨ğŸ¼â€ğŸ¤ ğŸ‘©ğŸ¼â€ğŸ« ğŸ‘¨ğŸ¼â€ğŸ« ğŸ‘©ğŸ¼â€ğŸ­ ğŸ‘¨ğŸ¼â€ğŸ­ ğŸ‘©ğŸ¼â€ğŸ’» ğŸ‘¨ğŸ¼â€ğŸ’» ğŸ‘©ğŸ¼â€ğŸ’¼ ğŸ‘¨ğŸ¼â€ğŸ’¼ ğŸ‘©ğŸ¼â€ğŸ”§ ğŸ‘¨ğŸ¼â€ğŸ”§ ğŸ‘©ğŸ¼â€ğŸ”¬ ğŸ‘¨ğŸ¼â€ğŸ”¬ ğŸ‘©ğŸ¼â€ğŸ¨ ğŸ‘¨ğŸ¼â€ğŸ¨ ğŸ‘©ğŸ¼â€ğŸš’ ğŸ‘¨ğŸ¼â€ğŸš’ ğŸ‘©ğŸ¼â€âœˆï¸ ğŸ‘¨ğŸ¼â€âœˆï¸ ğŸ‘©ğŸ¼â€ğŸš€ ğŸ‘¨ğŸ¼â€ğŸš€ ğŸ‘©ğŸ¼â€âš–ï¸ ğŸ‘¨ğŸ¼â€âš–ï¸ ğŸ¤¶ğŸ¼ ğŸ…ğŸ¼ ğŸ‘¸ğŸ¼ ğŸ¤´ğŸ¼ ğŸ‘°ğŸ¼ ğŸ¤µğŸ¼ ğŸ‘¼ğŸ¼ ğŸ¤°ğŸ¼ ğŸ™‡ğŸ¼â€â™€ï¸ ğŸ™‡ğŸ¼ ğŸ’ğŸ¼ ğŸ’ğŸ¼â€â™‚ï¸ ğŸ™…ğŸ¼ ğŸ™…ğŸ¼â€â™‚ï¸ ğŸ™†ğŸ¼ ğŸ™†ğŸ¼â€â™‚ï¸ ğŸ™‹ğŸ¼ ğŸ™‹ğŸ¼â€â™‚ï¸ ğŸ¤¦ğŸ¼â€â™€ï¸ ğŸ¤¦ğŸ¼â€â™‚ï¸ ğŸ¤·ğŸ¼â€â™€ï¸ ğŸ¤·ğŸ¼â€â™‚ï¸ ğŸ™ğŸ¼ ğŸ™ğŸ¼â€â™‚ï¸ ğŸ™ğŸ¼ ğŸ™ğŸ¼â€â™‚ï¸ ğŸ’‡ğŸ¼ ğŸ’‡ğŸ¼â€â™‚ï¸ ğŸ’†ğŸ¼ ğŸ’†ğŸ¼â€â™‚ï¸ ğŸ•´ğŸ¼ ğŸ’ƒğŸ¼ ğŸ•ºğŸ¼ ğŸš¶ğŸ¼â€â™€ï¸ ğŸš¶ğŸ¼ ğŸƒğŸ¼â€â™€ï¸ ğŸƒğŸ¼ ğŸ‹ğŸ¼â€â™€ï¸ ğŸ‹ğŸ¼ ğŸ¤¸ğŸ¼â€â™€ï¸ ğŸ¤¸ğŸ¼â€â™‚ï¸ â›¹ğŸ¼â€â™€ï¸ â›¹ğŸ¼ ğŸ¤¾ğŸ¼â€â™€ï¸ ğŸ¤¾ğŸ¼â€â™‚ï¸ ğŸŒğŸ¼â€â™€ï¸ ğŸŒğŸ¼ ğŸ„ğŸ¼â€â™€ï¸ ğŸ„ğŸ¼ ğŸŠğŸ¼â€â™€ï¸ ğŸŠğŸ¼ ğŸ¤½ğŸ¼â€â™€ï¸ ğŸ¤½ğŸ¼â€â™‚ï¸ ğŸš£ğŸ¼â€â™€ï¸ ğŸš£ğŸ¼ ğŸ‡ğŸ¼ ğŸš´ğŸ¼â€â™€ï¸ ğŸš´ğŸ¼ ğŸšµğŸ¼â€â™€ï¸ ğŸšµğŸ» ğŸ¤¹ğŸ¼â€â™€ï¸ ğŸ¤¹ğŸ¼â€â™‚ï¸ ğŸ›€ğŸ¼\n",
    "\n",
    "ğŸ‘ğŸ½ ğŸ™ŒğŸ½ ğŸ‘ğŸ½ ğŸ™ğŸ½ ğŸ‘ğŸ½ ğŸ‘ğŸ½ ğŸ‘ŠğŸ½ âœŠğŸ½ ğŸ¤›ğŸ½ ğŸ¤œğŸ½ ğŸ¤ğŸ½ âœŒğŸ½ ğŸ¤˜ğŸ½ ğŸ‘ŒğŸ½ ğŸ‘ˆğŸ½ ğŸ‘‰ğŸ½ ğŸ‘†ğŸ½ ğŸ‘‡ğŸ½ â˜ğŸ½ âœ‹ğŸ½ ğŸ¤šğŸ½ ğŸ–ğŸ½ ğŸ––ğŸ½ ğŸ‘‹ğŸ½ ğŸ¤™ğŸ½ ğŸ’ªğŸ½ ğŸ–•ğŸ½ âœğŸ½ ğŸ¤³ğŸ½ ğŸ’…ğŸ½ ğŸ‘‚ğŸ½ ğŸ‘ƒğŸ½ ğŸ‘¶ğŸ½ ğŸ‘¦ğŸ½ ğŸ‘§ğŸ½ ğŸ‘¨ğŸ½ ğŸ‘©ğŸ½ ğŸ‘±ğŸ½â€â™€ï¸ ğŸ‘±ğŸ½ ğŸ‘´ğŸ½ ğŸ‘µğŸ½ ğŸ‘²ğŸ½ ğŸ‘³ğŸ½â€â™€ï¸ ğŸ‘³ğŸ½ ğŸ‘®ğŸ½â€â™€ï¸ ğŸ‘®ğŸ½ ğŸ‘·ğŸ½â€â™€ï¸ ğŸ‘·ğŸ½ ğŸ’‚ğŸ½â€â™€ï¸ ğŸ’‚ğŸ½ ğŸ•µğŸ½â€â™€ï¸ ğŸ•µğŸ½ ğŸ‘©ğŸ½â€âš•ï¸ ğŸ‘¨ğŸ½â€âš•ï¸ ğŸ‘©ğŸ½â€ğŸŒ¾ ğŸ‘¨ğŸ½â€ğŸŒ¾ ğŸ‘©ğŸ½â€ğŸ³ ğŸ‘¨ğŸ½â€ğŸ³ ğŸ‘©ğŸ½â€ğŸ“ ğŸ‘¨ğŸ½â€ğŸ“ ğŸ‘©ğŸ½â€ğŸ¤ ğŸ‘¨ğŸ½â€ğŸ¤ ğŸ‘©ğŸ½â€ğŸ« ğŸ‘¨ğŸ½â€ğŸ« ğŸ‘©ğŸ½â€ğŸ­ ğŸ‘¨ğŸ½â€ğŸ­ ğŸ‘©ğŸ½â€ğŸ’» ğŸ‘¨ğŸ½â€ğŸ’» ğŸ‘©ğŸ½â€ğŸ’¼ ğŸ‘¨ğŸ½â€ğŸ’¼ ğŸ‘©ğŸ½â€ğŸ”§ ğŸ‘¨ğŸ½â€ğŸ”§ ğŸ‘©ğŸ½â€ğŸ”¬ ğŸ‘¨ğŸ½â€ğŸ”¬ ğŸ‘©ğŸ½â€ğŸ¨ ğŸ‘¨ğŸ½â€ğŸ¨ ğŸ‘©ğŸ½â€ğŸš’ ğŸ‘¨ğŸ½â€ğŸš’ ğŸ‘©ğŸ½â€âœˆï¸ ğŸ‘¨ğŸ½â€âœˆï¸ ğŸ‘©ğŸ½â€ğŸš€ ğŸ‘¨ğŸ½â€ğŸš€ ğŸ‘©ğŸ½â€âš–ï¸ ğŸ‘¨ğŸ½â€âš–ï¸ ğŸ¤¶ğŸ½ ğŸ…ğŸ½ ğŸ‘¸ğŸ½ ğŸ¤´ğŸ½ ğŸ‘°ğŸ½ ğŸ¤µğŸ½ ğŸ‘¼ğŸ½ ğŸ¤°ğŸ½ ğŸ™‡ğŸ½â€â™€ï¸ ğŸ™‡ğŸ½ ğŸ’ğŸ½ ğŸ’ğŸ½â€â™‚ï¸ ğŸ™…ğŸ½ ğŸ™…ğŸ½â€â™‚ï¸ ğŸ™†ğŸ½ ğŸ™†ğŸ½â€â™‚ï¸ ğŸ™‹ğŸ½ ğŸ™‹ğŸ½â€â™‚ï¸ ğŸ¤¦ğŸ½â€â™€ï¸ ğŸ¤¦ğŸ½â€â™‚ï¸ ğŸ¤·ğŸ½â€â™€ï¸ ğŸ¤·ğŸ½â€â™‚ï¸ ğŸ™ğŸ½ ğŸ™ğŸ½â€â™‚ï¸ ğŸ™ğŸ½ ğŸ™ğŸ½â€â™‚ï¸ ğŸ’‡ğŸ½ ğŸ’‡ğŸ½â€â™‚ï¸ ğŸ’†ğŸ½ ğŸ’†ğŸ½â€â™‚ï¸ ğŸ•´ğŸ¼ ğŸ’ƒğŸ½ ğŸ•ºğŸ½ ğŸš¶ğŸ½â€â™€ï¸ ğŸš¶ğŸ½ ğŸƒğŸ½â€â™€ï¸ ğŸƒğŸ½ ğŸ‹ğŸ½â€â™€ï¸ ğŸ‹ğŸ½ ğŸ¤¸ğŸ½â€â™€ï¸ ğŸ¤¸ğŸ½â€â™‚ï¸ â›¹ğŸ½â€â™€ï¸ â›¹ğŸ½ ğŸ¤¾ğŸ½â€â™€ï¸ ğŸ¤¾ğŸ½â€â™‚ï¸ ğŸŒğŸ½â€â™€ï¸ ğŸŒğŸ½ ğŸ„ğŸ½â€â™€ï¸ ğŸ„ğŸ½ ğŸŠğŸ½â€â™€ï¸ ğŸŠğŸ½ ğŸ¤½ğŸ½â€â™€ï¸ ğŸ¤½ğŸ½â€â™‚ï¸ ğŸš£ğŸ½â€â™€ï¸ ğŸš£ğŸ½ ğŸ‡ğŸ½ ğŸš´ğŸ½â€â™€ï¸ ğŸš´ğŸ½ ğŸšµğŸ½â€â™€ï¸ ğŸšµğŸ½ ğŸ¤¹ğŸ½â€â™€ï¸ ğŸ¤¹ğŸ½â€â™‚ï¸ ğŸ›€ğŸ½\n",
    "\n",
    "ğŸ‘ğŸ¾ ğŸ™ŒğŸ¾ ğŸ‘ğŸ¾ ğŸ™ğŸ¾ ğŸ‘ğŸ¾ ğŸ‘ğŸ¾ ğŸ‘ŠğŸ¾ âœŠğŸ¾ ğŸ¤›ğŸ¾ ğŸ¤œğŸ¾ ğŸ¤ğŸ¾ âœŒğŸ¾ ğŸ¤˜ğŸ¾ ğŸ‘ŒğŸ¾ ğŸ‘ˆğŸ¾ ğŸ‘‰ğŸ¾ ğŸ‘†ğŸ¾ ğŸ‘‡ğŸ¾ â˜ğŸ¾ âœ‹ğŸ¾ ğŸ¤šğŸ¾ ğŸ–ğŸ¾ ğŸ––ğŸ¾ ğŸ‘‹ğŸ¾ ğŸ¤™ğŸ¾ ğŸ’ªğŸ¾ ğŸ–•ğŸ¾ âœğŸ¾ ğŸ¤³ğŸ¾ ğŸ’…ğŸ¾ ğŸ‘‚ğŸ¾ ğŸ‘ƒğŸ¾ ğŸ‘¶ğŸ¾ ğŸ‘¦ğŸ¾ ğŸ‘§ğŸ¾ ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¾ ğŸ‘±ğŸ¾â€â™€ï¸ ğŸ‘±ğŸ¾ ğŸ‘´ğŸ¾ ğŸ‘µğŸ¾ ğŸ‘²ğŸ¾ ğŸ‘³ğŸ¾â€â™€ï¸ ğŸ‘³ğŸ¾ ğŸ‘®ğŸ¾â€â™€ï¸ ğŸ‘®ğŸ¾ ğŸ‘·ğŸ¾â€â™€ï¸ ğŸ‘·ğŸ¾ ğŸ’‚ğŸ¾â€â™€ï¸ ğŸ’‚ğŸ¾ ğŸ•µğŸ¾â€â™€ï¸ ğŸ•µğŸ¾ ğŸ‘©ğŸ¾â€âš•ï¸ ğŸ‘¨ğŸ¾â€âš•ï¸ ğŸ‘©ğŸ¾â€ğŸŒ¾ ğŸ‘¨ğŸ¾â€ğŸŒ¾ ğŸ‘©ğŸ¾â€ğŸ³ ğŸ‘¨ğŸ¾â€ğŸ³ ğŸ‘©ğŸ¾â€ğŸ“ ğŸ‘¨ğŸ¾â€ğŸ“ ğŸ‘©ğŸ¾â€ğŸ¤ ğŸ‘¨ğŸ¾â€ğŸ¤ ğŸ‘©ğŸ¾â€ğŸ« ğŸ‘¨ğŸ¾â€ğŸ« ğŸ‘©ğŸ¾â€ğŸ­ ğŸ‘¨ğŸ¾â€ğŸ­ ğŸ‘©ğŸ¾â€ğŸ’» ğŸ‘¨ğŸ¾â€ğŸ’» ğŸ‘©ğŸ¾â€ğŸ’¼ ğŸ‘¨ğŸ¾â€ğŸ’¼ ğŸ‘©ğŸ¾â€ğŸ”§ ğŸ‘¨ğŸ¾â€ğŸ”§ ğŸ‘©ğŸ¾â€ğŸ”¬ ğŸ‘¨ğŸ¾â€ğŸ”¬ ğŸ‘©ğŸ¾â€ğŸ¨ ğŸ‘¨ğŸ¾â€ğŸ¨ ğŸ‘©ğŸ¾â€ğŸš’ ğŸ‘¨ğŸ¾â€ğŸš’ ğŸ‘©ğŸ¾â€âœˆï¸ ğŸ‘¨ğŸ¾â€âœˆï¸ ğŸ‘©ğŸ¾â€ğŸš€ ğŸ‘¨ğŸ¾â€ğŸš€ ğŸ‘©ğŸ¾â€âš–ï¸ ğŸ‘¨ğŸ¾â€âš–ï¸ ğŸ¤¶ğŸ¾ ğŸ…ğŸ¾ ğŸ‘¸ğŸ¾ ğŸ¤´ğŸ¾ ğŸ‘°ğŸ¾ ğŸ¤µğŸ¾ ğŸ‘¼ğŸ¾ ğŸ¤°ğŸ¾ ğŸ™‡ğŸ¾â€â™€ï¸ ğŸ™‡ğŸ¾ ğŸ’ğŸ¾ ğŸ’ğŸ¾â€â™‚ï¸ ğŸ™…ğŸ¾ ğŸ™…ğŸ¾â€â™‚ï¸ ğŸ™†ğŸ¾ ğŸ™†ğŸ¾â€â™‚ï¸ ğŸ™‹ğŸ¾ ğŸ™‹ğŸ¾â€â™‚ï¸ ğŸ¤¦ğŸ¾â€â™€ï¸ ğŸ¤¦ğŸ¾â€â™‚ï¸ ğŸ¤·ğŸ¾â€â™€ï¸ ğŸ¤·ğŸ¾â€â™‚ï¸ ğŸ™ğŸ¾ ğŸ™ğŸ¾â€â™‚ï¸ ğŸ™ğŸ¾ ğŸ™ğŸ¾â€â™‚ï¸ ğŸ’‡ğŸ¾ ğŸ’‡ğŸ¾â€â™‚ï¸ ğŸ’†ğŸ¾ ğŸ’†ğŸ¾â€â™‚ï¸ ğŸ•´ğŸ¾ ğŸ’ƒğŸ¾ ğŸ•ºğŸ¾ ğŸš¶ğŸ¾â€â™€ï¸ ğŸš¶ğŸ¾ ğŸƒğŸ¾â€â™€ï¸ ğŸƒğŸ¾ ğŸ‹ğŸ¾â€â™€ï¸ ğŸ‹ğŸ¾ ğŸ¤¸ğŸ¾â€â™€ï¸ ğŸ¤¸ğŸ¾â€â™‚ï¸ â›¹ğŸ¾â€â™€ï¸ â›¹ğŸ¾ ğŸ¤¾ğŸ¾â€â™€ï¸ ğŸ¤¾ğŸ¾â€â™‚ï¸ ğŸŒğŸ¾â€â™€ï¸ ğŸŒğŸ¾ ğŸ„ğŸ¾â€â™€ï¸ ğŸ„ğŸ¾ ğŸŠğŸ¾â€â™€ï¸ ğŸŠğŸ¾ ğŸ¤½ğŸ¾â€â™€ï¸ ğŸ¤½ğŸ¾â€â™‚ï¸ ğŸš£ğŸ¾â€â™€ï¸ ğŸš£ğŸ¾ ğŸ‡ğŸ¾ ğŸš´ğŸ¾â€â™€ï¸ ğŸš´ğŸ¾ ğŸšµğŸ¾â€â™€ï¸ ğŸšµğŸ¾ ğŸ¤¹ğŸ¾â€â™€ï¸ ğŸ¤¹ğŸ¾â€â™‚ï¸ ğŸ›€ğŸ¾\n",
    "\n",
    "ğŸ‘ğŸ¿ ğŸ™ŒğŸ¿ ğŸ‘ğŸ¿ ğŸ™ğŸ¿ ğŸ‘ğŸ¿ ğŸ‘ğŸ¿ ğŸ‘ŠğŸ¿ âœŠğŸ¿ ğŸ¤›ğŸ¿ ğŸ¤œğŸ¿ ğŸ¤ğŸ¿ âœŒğŸ¿ ğŸ¤˜ğŸ¿ ğŸ‘ŒğŸ¿ ğŸ‘ˆğŸ¿ ğŸ‘‰ğŸ¿ ğŸ‘†ğŸ¿ ğŸ‘‡ğŸ¿ â˜ğŸ¿ âœ‹ğŸ¿ ğŸ¤šğŸ¿ ğŸ–ğŸ¿ ğŸ––ğŸ¿ ğŸ‘‹ğŸ¿ ğŸ¤™ğŸ¿ ğŸ’ªğŸ¿ ğŸ–•ğŸ¿ âœğŸ¿ ğŸ¤³ğŸ¿ ğŸ’…ğŸ¿ ğŸ‘‚ğŸ¿ ğŸ‘ƒğŸ¿ ğŸ‘¶ğŸ¿ ğŸ‘¦ğŸ¿ ğŸ‘§ğŸ¿ ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¿ ğŸ‘±ğŸ¿â€â™€ï¸ ğŸ‘±ğŸ¿ ğŸ‘´ğŸ¿ ğŸ‘µğŸ¿ ğŸ‘²ğŸ¿ ğŸ‘³ğŸ¿â€â™€ï¸ ğŸ‘³ğŸ¿ ğŸ‘®ğŸ¿â€â™€ï¸ ğŸ‘®ğŸ¿ ğŸ‘·ğŸ¿â€â™€ï¸ ğŸ‘·ğŸ¿ ğŸ’‚ğŸ¿â€â™€ï¸ ğŸ’‚ğŸ¿ ğŸ•µğŸ¿â€â™€ï¸ ğŸ•µğŸ¿ ğŸ‘©ğŸ¿â€âš•ï¸ ğŸ‘¨ğŸ¿â€âš•ï¸ ğŸ‘©ğŸ¿â€ğŸŒ¾ ğŸ‘¨ğŸ¿â€ğŸŒ¾ ğŸ‘©ğŸ¿â€ğŸ³ ğŸ‘¨ğŸ¿â€ğŸ³ ğŸ‘©ğŸ¿â€ğŸ“ ğŸ‘¨ğŸ¿â€ğŸ“ ğŸ‘©ğŸ¿â€ğŸ¤ ğŸ‘¨ğŸ¿â€ğŸ¤ ğŸ‘©ğŸ¿â€ğŸ« ğŸ‘¨ğŸ¿â€ğŸ« ğŸ‘©ğŸ¿â€ğŸ­ ğŸ‘¨ğŸ¿â€ğŸ­ ğŸ‘©ğŸ¿â€ğŸ’» ğŸ‘¨ğŸ¿â€ğŸ’» ğŸ‘©ğŸ¿â€ğŸ’¼ ğŸ‘¨ğŸ¿â€ğŸ’¼ ğŸ‘©ğŸ¿â€ğŸ”§ ğŸ‘¨ğŸ¿â€ğŸ”§ ğŸ‘©ğŸ¿â€ğŸ”¬ ğŸ‘¨ğŸ¿â€ğŸ”¬ ğŸ‘©ğŸ¿â€ğŸ¨ ğŸ‘¨ğŸ¿â€ğŸ¨ ğŸ‘©ğŸ¿â€ğŸš’ ğŸ‘¨ğŸ¿â€ğŸš’ ğŸ‘©ğŸ¿â€âœˆï¸ ğŸ‘¨ğŸ¿â€âœˆï¸ ğŸ‘©ğŸ¿â€ğŸš€ ğŸ‘¨ğŸ¿â€ğŸš€ ğŸ‘©ğŸ¿â€âš–ï¸ ğŸ‘¨ğŸ¿â€âš–ï¸ ğŸ¤¶ğŸ¿ ğŸ…ğŸ¿ ğŸ‘¸ğŸ¿ ğŸ¤´ğŸ¿ ğŸ‘°ğŸ¿ ğŸ¤µğŸ¿ ğŸ‘¼ğŸ¿ ğŸ¤°ğŸ¿ ğŸ™‡ğŸ¿â€â™€ï¸ ğŸ™‡ğŸ¿ ğŸ’ğŸ¿ ğŸ’ğŸ¿â€â™‚ï¸ ğŸ™…ğŸ¿ ğŸ™…ğŸ¿â€â™‚ï¸ ğŸ™†ğŸ¿ ğŸ™†ğŸ¿â€â™‚ï¸ ğŸ™‹ğŸ¿ ğŸ™‹ğŸ¿â€â™‚ï¸ ğŸ¤¦ğŸ¿â€â™€ï¸ ğŸ¤¦ğŸ¿â€â™‚ï¸ ğŸ¤·ğŸ¿â€â™€ï¸ ğŸ¤·ğŸ¿â€â™‚ï¸ ğŸ™ğŸ¿ ğŸ™ğŸ¿â€â™‚ï¸ ğŸ™ğŸ¿ ğŸ™ğŸ¿â€â™‚ï¸ ğŸ’‡ğŸ¿ ğŸ’‡ğŸ¿â€â™‚ï¸ ğŸ’†ğŸ¿ ğŸ’†ğŸ¿â€â™‚ï¸ ğŸ•´ğŸ¿ ğŸ’ƒğŸ¿ ğŸ•ºğŸ¿ ğŸš¶ğŸ¿â€â™€ï¸ ğŸš¶ğŸ¿ ğŸƒğŸ¿â€â™€ï¸ ğŸƒğŸ¿ ğŸ‹ğŸ¿â€â™€ï¸ ğŸ‹ğŸ¿ ğŸ¤¸ğŸ¿â€â™€ï¸ ğŸ¤¸ğŸ¿â€â™‚ï¸ â›¹ğŸ¿â€â™€ï¸ â›¹ğŸ¿ ğŸ¤¾ğŸ¿â€â™€ï¸ ğŸ¤¾ğŸ¿â€â™‚ï¸ ğŸŒğŸ¿â€â™€ï¸ ğŸŒğŸ¿ ğŸ„ğŸ¿â€â™€ï¸ ğŸ„ğŸ¿ ğŸŠğŸ¿â€â™€ï¸ ğŸŠğŸ¿ ğŸ¤½ğŸ¿â€â™€ï¸ ğŸ¤½ğŸ¿â€â™‚ï¸ ğŸš£ğŸ¿â€â™€ï¸ ğŸš£ğŸ¿ ğŸ‡ğŸ¿ ğŸš´ğŸ¿â€â™€ï¸ ğŸš´ğŸ¿ ğŸšµğŸ¿â€â™€ï¸ ğŸšµğŸ¿ ğŸ¤¹ğŸ¿â€â™€ï¸ ğŸ¤¹ğŸ¿â€â™‚ï¸ ğŸ›€ğŸ¿\n",
    "\n",
    "ğŸ¶ ğŸ± ğŸ­ ğŸ¹ ğŸ° ğŸ¦Š ğŸ» ğŸ¼ ğŸ¨ ğŸ¯ ğŸ¦ ğŸ® ğŸ· ğŸ½ ğŸ¸ ğŸµ ğŸ™Š ğŸ™‰ ğŸ™Š ğŸ’ ğŸ” ğŸ§ ğŸ¦ ğŸ¤ ğŸ£ ğŸ¥ ğŸ¦† ğŸ¦… ğŸ¦‰ ğŸ¦‡ ğŸº ğŸ— ğŸ´ ğŸ¦„ ğŸ ğŸ› ğŸ¦‹ ğŸŒ ğŸš ğŸ ğŸœ ğŸ•· ğŸ•¸ ğŸ¢ ğŸ ğŸ¦ ğŸ¦‚ ğŸ¦€ ğŸ¦‘ ğŸ™ ğŸ¦ ğŸ  ğŸŸ ğŸ¡ ğŸ¬ ğŸ¦ˆ ğŸ³ ğŸ‹ ğŸŠ ğŸ† ğŸ… ğŸƒ ğŸ‚ ğŸ„ ğŸ¦Œ ğŸª ğŸ« ğŸ˜ ğŸ¦ ğŸ¦ ğŸ ğŸ– ğŸ ğŸ ğŸ‘ ğŸ• ğŸ© ğŸˆ ğŸ“ ğŸ¦ƒ ğŸ•Š ğŸ‡ ğŸ ğŸ€ ğŸ¿ ğŸ¾ ğŸ‰ ğŸ² ğŸŒµ ğŸ„ ğŸŒ² ğŸŒ³ ğŸŒ´ ğŸŒ± ğŸŒ¿ â˜˜ï¸ ğŸ€ ğŸ ğŸ‹ ğŸƒ ğŸ‚ ğŸ ğŸ„ ğŸŒ¾ ğŸ’ ğŸŒ· ğŸŒ¹ ğŸ¥€ ğŸŒ» ğŸŒ¼ ğŸŒ¸ ğŸŒº ğŸŒ ğŸŒ ğŸŒ ğŸŒ• ğŸŒ– ğŸŒ— ğŸŒ˜ ğŸŒ‘ ğŸŒ’ ğŸŒ“ ğŸŒ” ğŸŒš ğŸŒ ğŸŒ ğŸŒ› ğŸŒœ ğŸŒ™ ğŸ’« â­ï¸ ğŸŒŸ âœ¨ âš¡ï¸ ğŸ”¥ ğŸ’¥ â˜„ï¸ â˜€ï¸ ğŸŒ¤ â›…ï¸ ğŸŒ¥ ğŸŒ¦ ğŸŒˆ â˜ï¸ ğŸŒ§ â›ˆ ğŸŒ© ğŸŒ¨ â˜ƒï¸ â›„ï¸ â„ï¸ ğŸŒ¬ ğŸ’¨ ğŸŒª ğŸŒ« ğŸŒŠ ğŸ’§ ğŸ’¦ â˜”ï¸\n",
    "\n",
    "ğŸ ğŸ ğŸ ğŸŠ ğŸ‹ ğŸŒ ğŸ‰ ğŸ‡ ğŸ“ ğŸˆ ğŸ’ ğŸ‘ ğŸ ğŸ¥ ğŸ¥‘ ğŸ… ğŸ† ğŸ¥’ ğŸ¥• ğŸŒ½ ğŸŒ¶ ğŸ¥” ğŸ  ğŸŒ° ğŸ¥œ ğŸ¯ ğŸ¥ ğŸ ğŸ¥– ğŸ§€ ğŸ¥š ğŸ³ ğŸ¥“ ğŸ¥ ğŸ¤ ğŸ— ğŸ– ğŸ• ğŸŒ­ ğŸ” ğŸŸ ğŸ¥™ ğŸŒ® ğŸŒ¯ ğŸ¥— ğŸ¥˜ ğŸ ğŸœ ğŸ² ğŸ¥ ğŸ£ ğŸ± ğŸ› ğŸš ğŸ™ ğŸ˜ ğŸ¢ ğŸ¡ ğŸ§ ğŸ¨ ğŸ¦ ğŸ° ğŸ‚ ğŸ® ğŸ­ ğŸ¬ ğŸ« ğŸ¿ ğŸ© ğŸª ğŸ¥› ğŸ¼ â˜•ï¸ ğŸµ ğŸ¶ ğŸº ğŸ» ğŸ¥‚ ğŸ· ğŸ¥ƒ ğŸ¸ ğŸ¹ ğŸ¾ ğŸ¥„ ğŸ´ ğŸ½\n",
    "\n",
    "âš½ï¸ ğŸ€ ğŸˆ âš¾ï¸ ğŸ¾ ğŸ ğŸ‰ ğŸ± ğŸ“ ğŸ¸ ğŸ¥… ğŸ’ ğŸ‘ ğŸ â›³ï¸ ğŸ¹ ğŸ£ ğŸ¥Š ğŸ¥‹ â›¸ ğŸ¿ â›· ğŸ‚ ğŸ‹ï¸â€â™€ï¸ ğŸ‹ï¸ ğŸ¤º ğŸ¤¼â€â™€ï¸ ğŸ¤¼â€â™‚ï¸ ğŸ¤¸â€â™€ï¸ ğŸ¤¸â€â™‚ï¸ â›¹ï¸â€â™€ï¸ â›¹ï¸ ğŸ¤¾â€â™€ï¸ ğŸ¤¾â€â™‚ï¸ ğŸŒï¸â€â™€ï¸ ğŸŒï¸ ğŸ„â€â™€ï¸ ğŸ„ ğŸŠâ€â™€ï¸ ğŸŠ ğŸ¤½â€â™€ï¸ ğŸ¤½â€â™‚ï¸ ğŸš£â€â™€ï¸ ğŸš£ ğŸ‡ ğŸš´â€â™€ï¸ ğŸš´ ğŸšµâ€â™€ï¸ ğŸšµ ğŸ½ ğŸ… ğŸ– ğŸ¥‡ ğŸ¥ˆ ğŸ¥‰ ğŸ† ğŸµ ğŸ— ğŸ« ğŸŸ ğŸª ğŸ¤¹â€â™€ï¸ ğŸ¤¹â€â™‚ï¸ ğŸ­ ğŸ¨ ğŸ¬ ğŸ¤ ğŸ§ ğŸ¼ ğŸ¹ ğŸ¥ ğŸ· ğŸº ğŸ¸ ğŸ» ğŸ² ğŸ¯ ğŸ³ ğŸ® ğŸ°\n",
    "\n",
    "ğŸš— ğŸš• ğŸš™ ğŸšŒ ğŸš ğŸ ğŸš“ ğŸš‘ ğŸš’ ğŸš ğŸšš ğŸš› ğŸšœ ğŸ›´ ğŸš² ğŸ›µ ğŸ ğŸš¨ ğŸš” ğŸš ğŸš˜ ğŸš– ğŸš¡ ğŸš  ğŸšŸ ğŸšƒ ğŸš‹ ğŸš ğŸš ğŸš„ ğŸš… ğŸšˆ ğŸš‚ ğŸš† ğŸš‡ ğŸšŠ ğŸš‰ ğŸš ğŸ›© âœˆï¸ ğŸ›« ğŸ›¬ ğŸš€ ğŸ›° ğŸ’º ğŸ›¶ â›µï¸ ğŸ›¥ ğŸš¤ ğŸ›³ â›´ ğŸš¢ âš“ï¸ ğŸš§ â›½ï¸ ğŸš ğŸš¦ ğŸš¥ ğŸ—º ğŸ—¿ ğŸ—½ â›²ï¸ ğŸ—¼ ğŸ° ğŸ¯ ğŸŸ ğŸ¡ ğŸ¢ ğŸ  â›± ğŸ– ğŸ â›° ğŸ” ğŸ—» ğŸŒ‹ ğŸœ ğŸ• â›ºï¸ ğŸ›¤ ğŸ›£ ğŸ— ğŸ­ ğŸ  ğŸ¡ ğŸ˜ ğŸš ğŸ¢ ğŸ¬ ğŸ£ ğŸ¤ ğŸ¥ ğŸ¦ ğŸ¨ ğŸª ğŸ« ğŸ© ğŸ’’ ğŸ› â›ªï¸ ğŸ•Œ ğŸ• ğŸ•‹ â›© ğŸ—¾ ğŸ‘ ğŸ ğŸŒ… ğŸŒ„ ğŸŒ  ğŸ‡ ğŸ† ğŸŒ‡ ğŸŒ† ğŸ™ ğŸŒƒ ğŸŒŒ ğŸŒ‰ ğŸŒ\n",
    "\n",
    "âŒšï¸ ğŸ“± ğŸ“² ğŸ’» âŒ¨ï¸ ğŸ–¥ ğŸ–¨ ğŸ–± ğŸ–² ğŸ•¹ ğŸ—œ ğŸ’½ ğŸ’¾ ğŸ’¿ ğŸ“€ ğŸ“¼ ğŸ“· ğŸ“¸ ğŸ“¹ ğŸ¥ ğŸ“½ ğŸ ğŸ“ â˜ï¸ ğŸ“Ÿ ğŸ“  ğŸ“º ğŸ“» ğŸ™ ğŸš ğŸ› â± â² â° ğŸ•° âŒ›ï¸ â³ ğŸ“¡ ğŸ”‹ ğŸ”Œ ğŸ’¡ ğŸ”¦ ğŸ•¯ ğŸ—‘ ğŸ›¢ ğŸ’¸ ğŸ’µ ğŸ’´ ğŸ’¶ ğŸ’· ğŸ’° ğŸ’³ ğŸ’ âš–ï¸ ğŸ”§ ğŸ”¨ âš’ ğŸ›  â› ğŸ”© âš™ï¸ â›“ ğŸ”« ğŸ’£ ğŸ”ª ğŸ—¡ âš”ï¸ ğŸ›¡ ğŸš¬ âš°ï¸ âš±ï¸ ğŸº ğŸ”® ğŸ“¿ ğŸ’ˆ âš—ï¸ ğŸ”­ ğŸ”¬ ğŸ•³ ğŸ’Š ğŸ’‰ ğŸŒ¡ ğŸš½ ğŸš° ğŸš¿ ğŸ› ğŸ›€ ğŸ› ğŸ”‘ ğŸ— ğŸšª ğŸ›‹ ğŸ› ğŸ›Œ ğŸ–¼ ğŸ› ğŸ›’ ğŸ ğŸˆ ğŸ ğŸ€ ğŸŠ ğŸ‰ ğŸ ğŸ® ğŸ âœ‰ï¸ ğŸ“© ğŸ“¨ ğŸ“§ ğŸ’Œ ğŸ“¥ ğŸ“¤ ğŸ“¦ ğŸ· ğŸ“ª ğŸ“« ğŸ“¬ ğŸ“­ ğŸ“® ğŸ“¯ ğŸ“œ ğŸ“ƒ ğŸ“„ ğŸ“‘ ğŸ“Š ğŸ“ˆ ğŸ“‰ ğŸ—’ ğŸ—“ ğŸ“† ğŸ“… ğŸ“‡ ğŸ—ƒ ğŸ—³ ğŸ—„ ğŸ“‹ ğŸ“ ğŸ“‚ ğŸ—‚ ğŸ— ğŸ“° ğŸ““ ğŸ“” ğŸ“’ ğŸ“• ğŸ“— ğŸ“˜ ğŸ“™ ğŸ“š ğŸ“– ğŸ”– ğŸ”— ğŸ“ ğŸ–‡ ğŸ“ ğŸ“ ğŸ“Œ ğŸ“ ğŸ“Œ ğŸŒ ğŸ³ï¸ ğŸ´ ğŸ ğŸ³ï¸â€ğŸŒˆ âœ‚ï¸ ğŸ–Š ğŸ–‹ âœ’ï¸ ğŸ–Œ ğŸ– ğŸ“ âœï¸ ğŸ” ğŸ” ğŸ” ğŸ” ğŸ”’ ğŸ”“\n",
    "\n",
    "â¤ï¸ ğŸ’› ğŸ’š ğŸ’™ ğŸ’œ ğŸ–¤ ğŸ’” â£ï¸ ğŸ’• ğŸ’ ğŸ’“ ğŸ’— ğŸ’– ğŸ’˜ ğŸ’ ğŸ’Ÿ â˜®ï¸ âœï¸ â˜ªï¸ ğŸ•‰ â˜¸ï¸ âœ¡ï¸ ğŸ”¯ ğŸ• â˜¯ï¸ â˜¦ï¸ ğŸ› â› â™ˆï¸ â™‰ï¸ â™Šï¸ â™‹ï¸ â™Œï¸ â™ï¸ â™ï¸ â™ï¸ â™ï¸ â™‘ï¸ â™’ï¸ â™“ï¸ ğŸ†” âš›ï¸ ğŸ‰‘ â˜¢ï¸ â˜£ï¸ ğŸ“´ ğŸ“³ ğŸˆ¶ ğŸˆšï¸ ğŸˆ¸ ğŸˆº ğŸˆ·ï¸ âœ´ï¸ ğŸ†š ğŸ’® ğŸ‰ ãŠ™ï¸ ãŠ—ï¸ ğŸˆ´ ğŸˆµ ğŸˆ¹ ğŸˆ² ğŸ…°ï¸ ğŸ…±ï¸ ğŸ† ğŸ†‘ ğŸ…¾ï¸ ğŸ†˜ âŒ â­•ï¸ ğŸ›‘ â›”ï¸ ğŸ“› ğŸš« ğŸ’¯ ğŸ’¢ â™¨ï¸ ğŸš· ğŸš¯ ğŸš³ ğŸš± ğŸ” ğŸ“µ ğŸš­ â—ï¸ â• â“ â” â€¼ï¸ â‰ï¸ ğŸ”… ğŸ”† ã€½ï¸ âš ï¸ ğŸš¸ ğŸ”± âšœï¸ ğŸ”° â™»ï¸ âœ… ğŸˆ¯ï¸ ğŸ’¹ â‡ï¸ âœ³ï¸ â ğŸŒ ğŸ’  â“‚ï¸ ğŸŒ€ ğŸ’¤ ğŸ§ ğŸš¾ â™¿ï¸ ğŸ…¿ï¸ ğŸˆ³ ğŸˆ‚ï¸ ğŸ›‚ ğŸ›ƒ ğŸ›„ ğŸ›… ğŸš¹ ğŸšº ğŸš¼ ğŸš» ğŸš® ğŸ¦ ğŸ“¶ ğŸˆ ğŸ”£ â„¹ï¸ ğŸ”¤ ğŸ”¡ ğŸ”  ğŸ†– ğŸ†— ğŸ†™ ğŸ†’ ğŸ†• ğŸ†“ 0ï¸âƒ£ 1ï¸âƒ£ 2ï¸âƒ£ 3ï¸âƒ£ 4ï¸âƒ£ 5ï¸âƒ£ 6ï¸âƒ£ 7ï¸âƒ£ 8ï¸âƒ£ 9ï¸âƒ£ ğŸ”Ÿ ğŸ”¢ #ï¸âƒ£ *ï¸âƒ£ â–¶ï¸ â¸ â¯ â¹ âº â­ â® â© âª â« â¬ â—€ï¸ ğŸ”¼ ğŸ”½ â¡ï¸ â¬…ï¸ â¬†ï¸ â¬‡ï¸ â†—ï¸ â†˜ï¸ â†™ï¸ â†–ï¸ â†•ï¸ â†”ï¸ â†ªï¸ â†©ï¸ â¤´ï¸ â¤µï¸ ğŸ”€ ğŸ” ğŸ”‚ ğŸ”„ ğŸ”ƒ ğŸµ ğŸ¶ â• â– â— âœ–ï¸ ğŸ’² ğŸ’± â„¢ï¸ Â©ï¸ Â®ï¸ ã€°ï¸ â° â¿ ğŸ”š ğŸ”™ ğŸ”› ğŸ” âœ”ï¸ â˜‘ï¸ ğŸ”˜ âšªï¸ âš«ï¸ ğŸ”´ ğŸ”µ ğŸ”º ğŸ”» ğŸ”¸ ğŸ”¹ ğŸ”¶ ğŸ”· ğŸ”³ ğŸ”² â–ªï¸ â–«ï¸ â—¾ï¸ â—½ï¸ â—¼ï¸ â—»ï¸ â¬›ï¸ â¬œï¸ ğŸ”ˆ ğŸ”‡ ğŸ”‰ ğŸ”Š ğŸ”” ğŸ”• ğŸ“£ ğŸ“¢ ğŸ‘â€ğŸ—¨ ğŸ’¬ ğŸ’­ ğŸ—¯ â™ ï¸ â™£ï¸ â™¥ï¸ â™¦ï¸ ğŸƒ ğŸ´ ğŸ€„ï¸ ğŸ• ğŸ•‘ ğŸ•’ ğŸ•“ ğŸ•” ğŸ•• ğŸ•– ğŸ•— ğŸ•˜ ğŸ•™ ğŸ•š ğŸ•› ğŸ•œ ğŸ• ğŸ• ğŸ•Ÿ ğŸ•  ğŸ•¡ ğŸ•¢ ğŸ•£ ğŸ•¤ ğŸ•¥ ğŸ•¦ ğŸ•§\n",
    "\n",
    "ğŸ³ï¸ ğŸ´ ğŸ ğŸš© ğŸ³ï¸â€ğŸŒˆ ğŸ‡¦ğŸ‡« ğŸ‡¦ğŸ‡½ ğŸ‡¦ğŸ‡± ğŸ‡©ğŸ‡¿ ğŸ‡¦ğŸ‡¸ ğŸ‡¦ğŸ‡© ğŸ‡¦ğŸ‡´ ğŸ‡¦ğŸ‡® ğŸ‡¦ğŸ‡¶ ğŸ‡¦ğŸ‡¬ ğŸ‡¦ğŸ‡· ğŸ‡¦ğŸ‡² ğŸ‡¦ğŸ‡¼ ğŸ‡¦ğŸ‡º ğŸ‡¦ğŸ‡¹ ğŸ‡¦ğŸ‡¿ ğŸ‡§ğŸ‡¸ ğŸ‡§ğŸ‡­ ğŸ‡§ğŸ‡© ğŸ‡§ğŸ‡§ ğŸ‡§ğŸ‡¾ ğŸ‡§ğŸ‡ª ğŸ‡§ğŸ‡¿ ğŸ‡§ğŸ‡¯ ğŸ‡§ğŸ‡² ğŸ‡§ğŸ‡¹ ğŸ‡§ğŸ‡´ ğŸ‡§ğŸ‡¦ ğŸ‡§ğŸ‡¼ ğŸ‡§ğŸ‡· ğŸ‡®ğŸ‡´ ğŸ‡»ğŸ‡¬ ğŸ‡§ğŸ‡³ ğŸ‡§ğŸ‡¬ ğŸ‡§ğŸ‡« ğŸ‡§ğŸ‡® ğŸ‡°ğŸ‡­ ğŸ‡¨ğŸ‡² ğŸ‡¨ğŸ‡¦ ğŸ‡®ğŸ‡¨ ğŸ‡¨ğŸ‡» ğŸ‡§ğŸ‡¶ ğŸ‡°ğŸ‡¾ ğŸ‡¨ğŸ‡« ğŸ‡¹ğŸ‡© ğŸ‡¨ğŸ‡± ğŸ‡¨ğŸ‡³ ğŸ‡¨ğŸ‡½ ğŸ‡¨ğŸ‡¨ ğŸ‡¨ğŸ‡´ ğŸ‡°ğŸ‡² ğŸ‡¨ğŸ‡¬ ğŸ‡¨ğŸ‡© ğŸ‡¨ğŸ‡° ğŸ‡¨ğŸ‡· ğŸ‡¨ğŸ‡® ğŸ‡­ğŸ‡· ğŸ‡¨ğŸ‡º ğŸ‡¨ğŸ‡¼ ğŸ‡¨ğŸ‡¾ ğŸ‡¨ğŸ‡¿ ğŸ‡©ğŸ‡° ğŸ‡©ğŸ‡¯ ğŸ‡©ğŸ‡² ğŸ‡©ğŸ‡´ ğŸ‡ªğŸ‡¨ ğŸ‡ªğŸ‡¬ ğŸ‡¸ğŸ‡» ğŸ‡¬ğŸ‡¶ ğŸ‡ªğŸ‡· ğŸ‡ªğŸ‡ª ğŸ‡ªğŸ‡¹ ğŸ‡ªğŸ‡º ğŸ‡«ğŸ‡° ğŸ‡«ğŸ‡´ ğŸ‡«ğŸ‡¯ ğŸ‡«ğŸ‡® ğŸ‡«ğŸ‡· ğŸ‡¬ğŸ‡« ğŸ‡µğŸ‡« ğŸ‡¹ğŸ‡« ğŸ‡¬ğŸ‡¦ ğŸ‡¬ğŸ‡² ğŸ‡¬ğŸ‡ª ğŸ‡©ğŸ‡ª ğŸ‡¬ğŸ‡­ ğŸ‡¬ğŸ‡® ğŸ‡¬ğŸ‡· ğŸ‡¬ğŸ‡± ğŸ‡¬ğŸ‡© ğŸ‡¬ğŸ‡µ ğŸ‡¬ğŸ‡º ğŸ‡¬ğŸ‡¹ ğŸ‡¬ğŸ‡¬ ğŸ‡¬ğŸ‡³ ğŸ‡¬ğŸ‡¼ ğŸ‡¬ğŸ‡¾ ğŸ‡­ğŸ‡¹ ğŸ‡­ğŸ‡³ ğŸ‡­ğŸ‡° ğŸ‡­ğŸ‡º ğŸ‡®ğŸ‡¸ ğŸ‡®ğŸ‡³ ğŸ‡®ğŸ‡© ğŸ‡®ğŸ‡· ğŸ‡®ğŸ‡¶ ğŸ‡®ğŸ‡ª ğŸ‡®ğŸ‡² ğŸ‡®ğŸ‡± ğŸ‡®ğŸ‡¹ ğŸ‡¯ğŸ‡² ğŸ‡¯ğŸ‡µ ğŸŒ ğŸ‡¯ğŸ‡ª ğŸ‡¯ğŸ‡´ ğŸ‡°ğŸ‡¿ ğŸ‡°ğŸ‡ª ğŸ‡°ğŸ‡® ğŸ‡½ğŸ‡° ğŸ‡°ğŸ‡¼ ğŸ‡°ğŸ‡¬ ğŸ‡±ğŸ‡¦ ğŸ‡±ğŸ‡» ğŸ‡±ğŸ‡§ ğŸ‡±ğŸ‡¸ ğŸ‡±ğŸ‡· ğŸ‡±ğŸ‡¾ ğŸ‡±ğŸ‡® ğŸ‡±ğŸ‡¹ ğŸ‡±ğŸ‡º ğŸ‡²ğŸ‡´ ğŸ‡²ğŸ‡° ğŸ‡²ğŸ‡¬ ğŸ‡²ğŸ‡¼ ğŸ‡²ğŸ‡¾ ğŸ‡²ğŸ‡» ğŸ‡²ğŸ‡± ğŸ‡²ğŸ‡¹ ğŸ‡²ğŸ‡­ ğŸ‡²ğŸ‡¶ ğŸ‡²ğŸ‡· ğŸ‡²ğŸ‡º ğŸ‡¾ğŸ‡¹ ğŸ‡²ğŸ‡½ ğŸ‡«ğŸ‡² ğŸ‡²ğŸ‡© ğŸ‡²ğŸ‡¨ ğŸ‡²ğŸ‡³ ğŸ‡²ğŸ‡ª ğŸ‡²ğŸ‡¸ ğŸ‡²ğŸ‡¦ ğŸ‡²ğŸ‡¿ ğŸ‡²ğŸ‡² ğŸ‡³ğŸ‡¦ ğŸ‡³ğŸ‡· ğŸ‡³ğŸ‡µ ğŸ‡³ğŸ‡± ğŸ‡³ğŸ‡¨ ğŸ‡³ğŸ‡¿ ğŸ‡³ğŸ‡® ğŸ‡³ğŸ‡ª ğŸ‡³ğŸ‡¬ ğŸ‡³ğŸ‡º ğŸ‡³ğŸ‡« ğŸ‡°ğŸ‡µ ğŸ‡²ğŸ‡µ ğŸ‡³ğŸ‡´ ğŸ‡´ğŸ‡² ğŸ‡µğŸ‡° ğŸ‡µğŸ‡¼ ğŸ‡µğŸ‡¸ ğŸ‡µğŸ‡¦ ğŸ‡µğŸ‡¬ ğŸ‡µğŸ‡¾ ğŸ‡µğŸ‡ª ğŸ‡µğŸ‡­ ğŸ‡µğŸ‡³ ğŸ‡µğŸ‡± ğŸ‡µğŸ‡¹ ğŸ‡µğŸ‡· ğŸ‡¶ğŸ‡¦ ğŸ‡·ğŸ‡ª ğŸ‡·ğŸ‡´ ğŸ‡·ğŸ‡º ğŸ‡·ğŸ‡¼ ğŸ‡¼ğŸ‡¸ ğŸ‡¸ğŸ‡² ğŸ‡¸ğŸ‡¦ ğŸ‡¸ğŸ‡³ ğŸ‡·ğŸ‡¸ ğŸ‡¸ğŸ‡¨ ğŸ‡¸ğŸ‡± ğŸ‡¸ğŸ‡¬ ğŸ‡¸ğŸ‡½ ğŸ‡¸ğŸ‡° ğŸ‡¸ğŸ‡® ğŸ‡¬ğŸ‡¸ ğŸ‡¸ğŸ‡§ ğŸ‡¸ğŸ‡´ ğŸ‡¿ğŸ‡¦ ğŸ‡°ğŸ‡· ğŸ‡¸ğŸ‡¸ ğŸ‡ªğŸ‡¸ ğŸ‡±ğŸ‡° ğŸ‡§ğŸ‡± ğŸ‡¸ğŸ‡­ ğŸ‡°ğŸ‡³ ğŸ‡±ğŸ‡¨ ğŸ‡µğŸ‡² ğŸ‡»ğŸ‡¨ ğŸ‡¸ğŸ‡© ğŸ‡¸ğŸ‡· ğŸ‡¸ğŸ‡¿ ğŸ‡¸ğŸ‡ª ğŸ‡¨ğŸ‡­ ğŸ‡¸ğŸ‡¾ ğŸ‡¹ğŸ‡¼ ğŸ‡¹ğŸ‡¯ ğŸ‡¹ğŸ‡¿ ğŸ‡¹ğŸ‡­ ğŸ‡¹ğŸ‡± ğŸ‡¹ğŸ‡¬ ğŸ‡¹ğŸ‡° ğŸ‡¹ğŸ‡´ ğŸ‡¹ğŸ‡¹ ğŸ‡¹ğŸ‡³ ğŸ‡¹ğŸ‡· ğŸ‡¹ğŸ‡² ğŸ‡¹ğŸ‡¨ ğŸ‡¹ğŸ‡» ğŸ‡»ğŸ‡® ğŸ‡ºğŸ‡¬ ğŸ‡ºğŸ‡¦ ğŸ‡¦ğŸ‡ª ğŸ‡¬ğŸ‡§ ğŸ´ó §ó ¢ó ¥ó ®ó §ó ¿ ğŸ´ó §ó ¢ó ³ó £ó ´ó ¿ ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ ğŸ‡ºğŸ‡¸ ğŸ‡ºğŸ‡¾ ğŸ‡ºğŸ‡¿ ğŸ‡»ğŸ‡º ğŸ‡»ğŸ‡¦ ğŸ‡»ğŸ‡ª ğŸ‡»ğŸ‡³ ğŸ‡¼ğŸ‡« ğŸ‡ªğŸ‡­ ğŸ‡¾ğŸ‡ª ğŸ‡¿ğŸ‡² ğŸ‡¿ğŸ‡¼\n",
    "'''\n",
    "\n",
    "\n",
    "def clean(row, string_with_chars_to_remove):\n",
    "    \n",
    "    # Cria uma lista de palavras\n",
    "    words = row.tweet.split()\n",
    "    \n",
    "    # Remove hashtags, mentions e links\n",
    "    words = [word for word in words if ( (word[0] not in ['#', '@']) \n",
    "             and (word.startswith('http') is False)\n",
    "             and (word.startswith('www.') is False)\n",
    "             and (word.startswith('t.co') is False)\n",
    "             and (word.startswith('bit.ly') is False)\n",
    "             and (word.startswith('goo.gl') is False)\n",
    "             and (word.startswith('migre.me') is False) )\n",
    "            ]\n",
    "\n",
    "    # Remove qualquer termo que nÃ£o contenha nenhum caractere do alfabeto, incluindo acentos\n",
    "    words = [word for word in words if any(letter.isalpha() for letter in word)]\n",
    "    \n",
    "    # Remove pontuaÃ§Ã£o do comeÃ§o e final das palavras\n",
    "    for i in range(len(words)):\n",
    "        words[i] = words[i].strip((string.punctuation + emojistring))\n",
    "    \n",
    "    # Remove emojis do meio de palavras\n",
    "    # https://gist.github.com/Alex-Just/e86110836f3f93fe7932290526529cd1\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    for i in range(len(words)):\n",
    "        words[i] = RE_EMOJI.sub(r'', words[i])\n",
    "        \n",
    "    text = ' '.join(words)\n",
    "     \n",
    "    return pd.Series({'tweet':text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_classificados.csv')\n",
    "\n",
    "data = []\n",
    "clean_data = []\n",
    "data_labels = []\n",
    "\n",
    "df['clean_tweet'] = df.apply(clean, args=(emojistring,), axis=1)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    data.append(row['tweet'])\n",
    "    clean_data.append(row['clean_tweet'])\n",
    "    if row['review'] == 1:\n",
    "        data_labels.append('pos')\n",
    "    else:\n",
    "        data_labels.append('neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uma anÃ¡lise da quantidade de tweets em cada uma das classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1012"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Positivos\n",
    "len(df[df['review'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Negativos\n",
    "len(df[df['review'] == -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Neutros\n",
    "len(df[df['review'] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CriaÃ§Ã£o dos parÃ¢metros e classificadores\n",
    "\n",
    "Remover stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vc',\n",
       " 'de',\n",
       " 'a',\n",
       " 'o',\n",
       " 'que',\n",
       " 'e',\n",
       " 'do',\n",
       " 'da',\n",
       " 'em',\n",
       " 'um',\n",
       " 'para',\n",
       " 'Ã©',\n",
       " 'com',\n",
       " 'uma',\n",
       " 'os',\n",
       " 'no',\n",
       " 'se',\n",
       " 'na',\n",
       " 'por',\n",
       " 'mais',\n",
       " 'as',\n",
       " 'dos',\n",
       " 'como',\n",
       " 'mas',\n",
       " 'foi',\n",
       " 'ao',\n",
       " 'ele',\n",
       " 'das',\n",
       " 'tem',\n",
       " 'Ã ',\n",
       " 'seu',\n",
       " 'sua',\n",
       " 'ou',\n",
       " 'ser',\n",
       " 'quando',\n",
       " 'muito',\n",
       " 'hÃ¡',\n",
       " 'nos',\n",
       " 'jÃ¡',\n",
       " 'estÃ¡',\n",
       " 'eu',\n",
       " 'tambÃ©m',\n",
       " 'sÃ³',\n",
       " 'pelo',\n",
       " 'pela',\n",
       " 'atÃ©',\n",
       " 'isso',\n",
       " 'ela',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'depois',\n",
       " 'sem',\n",
       " 'mesmo',\n",
       " 'aos',\n",
       " 'ter',\n",
       " 'seus',\n",
       " 'quem',\n",
       " 'nas',\n",
       " 'me',\n",
       " 'esse',\n",
       " 'eles',\n",
       " 'estÃ£o',\n",
       " 'vocÃª',\n",
       " 'tinha',\n",
       " 'foram',\n",
       " 'essa',\n",
       " 'num',\n",
       " 'nem',\n",
       " 'suas',\n",
       " 'meu',\n",
       " 'Ã s',\n",
       " 'minha',\n",
       " 'tÃªm',\n",
       " 'numa',\n",
       " 'pelos',\n",
       " 'elas',\n",
       " 'havia',\n",
       " 'seja',\n",
       " 'qual',\n",
       " 'serÃ¡',\n",
       " 'nÃ³s',\n",
       " 'tenho',\n",
       " 'lhe',\n",
       " 'deles',\n",
       " 'essas',\n",
       " 'esses',\n",
       " 'pelas',\n",
       " 'este',\n",
       " 'fosse',\n",
       " 'dele',\n",
       " 'tu',\n",
       " 'te',\n",
       " 'vocÃªs',\n",
       " 'vos',\n",
       " 'lhes',\n",
       " 'meus',\n",
       " 'minhas',\n",
       " 'teu',\n",
       " 'tua',\n",
       " 'teus',\n",
       " 'tuas',\n",
       " 'nosso',\n",
       " 'nossa',\n",
       " 'nossos',\n",
       " 'nossas',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'esta',\n",
       " 'estes',\n",
       " 'estas',\n",
       " 'aquele',\n",
       " 'aquela',\n",
       " 'aqueles',\n",
       " 'aquelas',\n",
       " 'isto',\n",
       " 'aquilo',\n",
       " 'estou',\n",
       " 'estÃ¡',\n",
       " 'estamos',\n",
       " 'estÃ£o',\n",
       " 'estive',\n",
       " 'esteve',\n",
       " 'estivemos',\n",
       " 'estiveram',\n",
       " 'estava',\n",
       " 'estÃ¡vamos',\n",
       " 'estavam',\n",
       " 'estivera',\n",
       " 'estivÃ©ramos',\n",
       " 'esteja',\n",
       " 'estejamos',\n",
       " 'estejam',\n",
       " 'estivesse',\n",
       " 'estivÃ©ssemos',\n",
       " 'estivessem',\n",
       " 'estiver',\n",
       " 'estivermos',\n",
       " 'estiverem',\n",
       " 'hei',\n",
       " 'hÃ¡',\n",
       " 'havemos',\n",
       " 'hÃ£o',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houveram',\n",
       " 'houvera',\n",
       " 'houvÃ©ramos',\n",
       " 'haja',\n",
       " 'hajamos',\n",
       " 'hajam',\n",
       " 'houvesse',\n",
       " 'houvÃ©ssemos',\n",
       " 'houvessem',\n",
       " 'houver',\n",
       " 'houvermos',\n",
       " 'houverem',\n",
       " 'houverei',\n",
       " 'houverÃ¡',\n",
       " 'houveremos',\n",
       " 'houverÃ£o',\n",
       " 'houveria',\n",
       " 'houverÃ­amos',\n",
       " 'houveriam',\n",
       " 'sou',\n",
       " 'somos',\n",
       " 'sÃ£o',\n",
       " 'era',\n",
       " 'Ã©ramos',\n",
       " 'eram',\n",
       " 'fui',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'foram',\n",
       " 'fora',\n",
       " 'fÃ´ramos',\n",
       " 'seja',\n",
       " 'sejamos',\n",
       " 'sejam',\n",
       " 'fosse',\n",
       " 'fÃ´ssemos',\n",
       " 'fossem',\n",
       " 'for',\n",
       " 'formos',\n",
       " 'forem',\n",
       " 'serei',\n",
       " 'serÃ¡',\n",
       " 'seremos',\n",
       " 'serÃ£o',\n",
       " 'seria',\n",
       " 'serÃ­amos',\n",
       " 'seriam',\n",
       " 'tenho',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 'tÃ©m',\n",
       " 'tinha',\n",
       " 'tÃ­nhamos',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'teve',\n",
       " 'tivemos',\n",
       " 'tiveram',\n",
       " 'tivera',\n",
       " 'tivÃ©ramos',\n",
       " 'tenha',\n",
       " 'tenhamos',\n",
       " 'tenham',\n",
       " 'tivesse',\n",
       " 'tivÃ©ssemos',\n",
       " 'tivessem',\n",
       " 'tiver',\n",
       " 'tivermos',\n",
       " 'tiverem',\n",
       " 'terei',\n",
       " 'terÃ¡',\n",
       " 'teremos',\n",
       " 'terÃ£o',\n",
       " 'teria',\n",
       " 'terÃ­amos',\n",
       " 'teriam']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = [line.rstrip() for line in open('stopwords.txt')]\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params(data, data_labels, ngram):\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "        analyzer = 'word',\n",
    "        lowercase = False,\n",
    "        stop_words = stopwords,\n",
    "        ngram_range=(ngram, ngram)\n",
    "    )\n",
    "\n",
    "    features = vectorizer.fit_transform(\n",
    "        data\n",
    "    )\n",
    "\n",
    "    features_nd = features.toarray()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features_nd, \n",
    "            data_labels,\n",
    "            train_size=0.9,\n",
    "#             random_state = 42  #  Usando random state fixo para testar\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumb_logit_classifier(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    log_model = LogisticRegression()\n",
    "    log_model = log_model.fit(X=X_train, y=y_train)\n",
    "    y_pred = log_model.predict(X_test)\n",
    "\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classification(X_train, X_valid, y_train, y_valid):\n",
    "    \n",
    "    from sklearn import svm\n",
    "    svm = svm.SVC()\n",
    "    svm.fit(X_train, y_train)  \n",
    "    print(svm.score(X_valid, y_valid))\n",
    "    return svm.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alguns resultados de acurÃ¡cia para a regressÃ£o logÃ­stica\n",
    "\n",
    "Para unigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7450980392156863\n",
      "0.7156862745098039\n",
      "0.7647058823529411\n",
      "0.7450980392156863\n",
      "0.7352941176470589\n",
      "0.7254901960784313\n",
      "0.7941176470588235\n",
      "0.7745098039215687\n",
      "0.7254901960784313\n",
      "0.6568627450980392\n",
      "0.7647058823529411\n",
      "0.7745098039215687\n",
      "0.7254901960784313\n",
      "0.7647058823529411\n",
      "0.7254901960784313\n",
      "0.7843137254901961\n",
      "0.7843137254901961\n",
      "0.7549019607843137\n",
      "0.7450980392156863\n",
      "0.7647058823529411\n",
      "0.7843137254901961\n",
      "0.7647058823529411\n",
      "0.803921568627451\n",
      "0.6764705882352942\n",
      "0.7843137254901961\n",
      "0.803921568627451\n",
      "0.7254901960784313\n",
      "0.8137254901960784\n",
      "0.803921568627451\n",
      "0.7745098039215687\n",
      "0.7352941176470589\n",
      "0.7058823529411765\n",
      "0.7941176470588235\n",
      "0.7941176470588235\n",
      "0.7941176470588235\n",
      "0.7745098039215687\n",
      "0.7843137254901961\n",
      "0.8137254901960784\n",
      "0.7843137254901961\n",
      "0.6862745098039216\n",
      "0.7254901960784313\n",
      "0.7549019607843137\n",
      "0.7156862745098039\n",
      "0.7450980392156863\n",
      "0.7156862745098039\n",
      "0.7156862745098039\n",
      "0.7843137254901961\n",
      "0.7745098039215687\n",
      "0.8137254901960784\n",
      "0.7549019607843137\n",
      "0.7843137254901961\n",
      "0.7549019607843137\n",
      "0.8235294117647058\n",
      "0.7450980392156863\n",
      "0.7352941176470589\n",
      "0.803921568627451\n",
      "0.7450980392156863\n",
      "0.7254901960784313\n",
      "0.8431372549019608\n",
      "0.7647058823529411\n",
      "0.7843137254901961\n",
      "0.7450980392156863\n",
      "0.803921568627451\n",
      "0.803921568627451\n",
      "0.7745098039215687\n",
      "0.7254901960784313\n",
      "0.7549019607843137\n",
      "0.7254901960784313\n",
      "0.7843137254901961\n",
      "0.7745098039215687\n",
      "0.7941176470588235\n",
      "0.7745098039215687\n",
      "0.7254901960784313\n",
      "0.7941176470588235\n",
      "0.7450980392156863\n",
      "0.7843137254901961\n",
      "0.7843137254901961\n",
      "0.7352941176470589\n",
      "0.803921568627451\n",
      "0.7254901960784313\n",
      "0.7745098039215687\n",
      "0.7549019607843137\n",
      "0.7352941176470589\n",
      "0.7647058823529411\n",
      "0.7745098039215687\n",
      "0.7745098039215687\n",
      "0.7941176470588235\n",
      "0.7647058823529411\n",
      "0.7156862745098039\n",
      "0.8333333333333334\n",
      "0.7843137254901961\n",
      "0.7549019607843137\n",
      "0.8235294117647058\n",
      "0.7647058823529411\n",
      "0.696078431372549\n",
      "0.7352941176470589\n",
      "0.7745098039215687\n",
      "0.7843137254901961\n",
      "0.803921568627451\n",
      "0.6470588235294118\n"
     ]
    }
   ],
   "source": [
    "values_logint_uni = []\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = create_params(data, data_labels, 1)\n",
    "    values_logint_uni.append(dumb_logit_classifier(X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7619607843137255"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(values_logint_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para bigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7843137254901961\n",
      "0.7352941176470589\n",
      "0.803921568627451\n",
      "0.8137254901960784\n",
      "0.8333333333333334\n",
      "0.7647058823529411\n",
      "0.7254901960784313\n",
      "0.7745098039215687\n",
      "0.7254901960784313\n",
      "0.7156862745098039\n",
      "0.8235294117647058\n",
      "0.7450980392156863\n",
      "0.7745098039215687\n",
      "0.803921568627451\n",
      "0.7647058823529411\n",
      "0.7549019607843137\n",
      "0.7156862745098039\n",
      "0.803921568627451\n",
      "0.7450980392156863\n",
      "0.7450980392156863\n",
      "0.8529411764705882\n",
      "0.8529411764705882\n",
      "0.8235294117647058\n",
      "0.7647058823529411\n",
      "0.7647058823529411\n",
      "0.7941176470588235\n",
      "0.8431372549019608\n",
      "0.7254901960784313\n",
      "0.8235294117647058\n",
      "0.7647058823529411\n",
      "0.7450980392156863\n",
      "0.7450980392156863\n",
      "0.7156862745098039\n",
      "0.7058823529411765\n",
      "0.7843137254901961\n",
      "0.7843137254901961\n",
      "0.6568627450980392\n",
      "0.7352941176470589\n",
      "0.7843137254901961\n",
      "0.7745098039215687\n",
      "0.7549019607843137\n",
      "0.7745098039215687\n",
      "0.8431372549019608\n",
      "0.8137254901960784\n",
      "0.7843137254901961\n",
      "0.803921568627451\n",
      "0.7941176470588235\n",
      "0.7156862745098039\n",
      "0.8137254901960784\n",
      "0.7549019607843137\n",
      "0.7941176470588235\n",
      "0.803921568627451\n",
      "0.7745098039215687\n",
      "0.7647058823529411\n",
      "0.7941176470588235\n",
      "0.8333333333333334\n",
      "0.7058823529411765\n",
      "0.7352941176470589\n",
      "0.7450980392156863\n",
      "0.7941176470588235\n",
      "0.7647058823529411\n",
      "0.696078431372549\n",
      "0.7843137254901961\n",
      "0.7745098039215687\n",
      "0.8725490196078431\n",
      "0.7549019607843137\n",
      "0.8333333333333334\n",
      "0.7352941176470589\n",
      "0.7450980392156863\n",
      "0.7450980392156863\n",
      "0.7843137254901961\n",
      "0.7647058823529411\n",
      "0.7058823529411765\n",
      "0.7549019607843137\n",
      "0.7647058823529411\n",
      "0.7941176470588235\n",
      "0.7745098039215687\n",
      "0.6862745098039216\n",
      "0.7549019607843137\n",
      "0.8137254901960784\n",
      "0.7745098039215687\n",
      "0.7450980392156863\n",
      "0.803921568627451\n",
      "0.7843137254901961\n",
      "0.7647058823529411\n",
      "0.7647058823529411\n",
      "0.696078431372549\n",
      "0.7941176470588235\n",
      "0.7647058823529411\n",
      "0.7745098039215687\n",
      "0.803921568627451\n",
      "0.6666666666666666\n",
      "0.7156862745098039\n",
      "0.803921568627451\n",
      "0.7647058823529411\n",
      "0.7745098039215687\n",
      "0.6862745098039216\n",
      "0.7843137254901961\n",
      "0.7254901960784313\n",
      "0.803921568627451\n"
     ]
    }
   ],
   "source": [
    "values_logit_bi = []\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = create_params(data, data_labels, 2)\n",
    "    values_logit_bi.append(dumb_logit_classifier(X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.768921568627451"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(values_logit_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alguns resultados de acurÃ¡cia para SVM\n",
    "\n",
    "Primeiramente para unigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7549019607843137\n",
      "0.7549019607843137\n",
      "0.7549019607843137\n",
      "0.7647058823529411\n",
      "0.7647058823529411\n",
      "0.8137254901960784\n",
      "0.7647058823529411\n",
      "0.803921568627451\n",
      "0.696078431372549\n",
      "0.7745098039215687\n",
      "0.7941176470588235\n",
      "0.6862745098039216\n",
      "0.7941176470588235\n",
      "0.8235294117647058\n",
      "0.7843137254901961\n",
      "0.7156862745098039\n",
      "0.7352941176470589\n",
      "0.7745098039215687\n",
      "0.803921568627451\n",
      "0.803921568627451\n",
      "0.7843137254901961\n",
      "0.7745098039215687\n",
      "0.8137254901960784\n",
      "0.7941176470588235\n",
      "0.8725490196078431\n",
      "0.8137254901960784\n",
      "0.7254901960784313\n",
      "0.7745098039215687\n",
      "0.7647058823529411\n",
      "0.7745098039215687\n",
      "0.7647058823529411\n",
      "0.7941176470588235\n",
      "0.7843137254901961\n",
      "0.7254901960784313\n",
      "0.803921568627451\n",
      "0.7647058823529411\n",
      "0.8235294117647058\n",
      "0.7549019607843137\n",
      "0.8627450980392157\n",
      "0.696078431372549\n",
      "0.7254901960784313\n",
      "0.7352941176470589\n",
      "0.7647058823529411\n",
      "0.7549019607843137\n",
      "0.7647058823529411\n",
      "0.803921568627451\n",
      "0.7352941176470589\n",
      "0.8137254901960784\n",
      "0.7352941176470589\n",
      "0.8627450980392157\n",
      "0.803921568627451\n",
      "0.7254901960784313\n",
      "0.7843137254901961\n",
      "0.8431372549019608\n",
      "0.696078431372549\n",
      "0.7549019607843137\n",
      "0.8137254901960784\n",
      "0.7647058823529411\n",
      "0.8137254901960784\n",
      "0.7647058823529411\n",
      "0.8333333333333334\n",
      "0.7941176470588235\n",
      "0.7843137254901961\n",
      "0.7941176470588235\n",
      "0.7941176470588235\n",
      "0.803921568627451\n",
      "0.8235294117647058\n",
      "0.803921568627451\n",
      "0.7549019607843137\n",
      "0.7549019607843137\n",
      "0.7745098039215687\n",
      "0.8137254901960784\n",
      "0.7843137254901961\n",
      "0.7450980392156863\n",
      "0.7156862745098039\n",
      "0.8137254901960784\n",
      "0.7843137254901961\n",
      "0.7352941176470589\n",
      "0.803921568627451\n",
      "0.8235294117647058\n",
      "0.7549019607843137\n",
      "0.803921568627451\n",
      "0.7549019607843137\n",
      "0.7254901960784313\n",
      "0.7941176470588235\n",
      "0.7941176470588235\n",
      "0.7647058823529411\n",
      "0.803921568627451\n",
      "0.7745098039215687\n",
      "0.7549019607843137\n",
      "0.7549019607843137\n",
      "0.8333333333333334\n",
      "0.8431372549019608\n",
      "0.7549019607843137\n",
      "0.7156862745098039\n",
      "0.8529411764705882\n",
      "0.7254901960784313\n",
      "0.7647058823529411\n",
      "0.8333333333333334\n",
      "0.7156862745098039\n"
     ]
    }
   ],
   "source": [
    "values_svm_uni = []\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = create_params(data, data_labels, 1)\n",
    "    values_svm_uni.append(svm_classification(X_train, X_test, y_train, y_test))\n",
    "print(mean(values_svm_uni))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora para bigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n",
      "0.7549019607843137\n",
      "0.8627450980392157\n",
      "0.7745098039215687\n",
      "0.8235294117647058\n",
      "0.7745098039215687\n",
      "0.803921568627451\n",
      "0.8725490196078431\n",
      "0.8137254901960784\n",
      "0.7450980392156863\n",
      "0.8235294117647058\n",
      "0.7450980392156863\n",
      "0.7647058823529411\n",
      "0.8627450980392157\n",
      "0.7745098039215687\n",
      "0.7745098039215687\n",
      "0.696078431372549\n",
      "0.7450980392156863\n",
      "0.7549019607843137\n",
      "0.7843137254901961\n",
      "0.7941176470588235\n",
      "0.8333333333333334\n",
      "0.7647058823529411\n",
      "0.7843137254901961\n",
      "0.7549019607843137\n",
      "0.7745098039215687\n",
      "0.7549019607843137\n",
      "0.7058823529411765\n",
      "0.8627450980392157\n",
      "0.7156862745098039\n",
      "0.7156862745098039\n",
      "0.8431372549019608\n",
      "0.7843137254901961\n",
      "0.8431372549019608\n",
      "0.7450980392156863\n",
      "0.7745098039215687\n",
      "0.7941176470588235\n",
      "0.803921568627451\n",
      "0.7941176470588235\n",
      "0.8235294117647058\n",
      "0.6372549019607843\n",
      "0.8235294117647058\n",
      "0.8529411764705882\n",
      "0.7254901960784313\n",
      "0.7254901960784313\n",
      "0.8137254901960784\n",
      "0.7254901960784313\n",
      "0.8137254901960784\n",
      "0.8137254901960784\n",
      "0.7647058823529411\n",
      "0.7647058823529411\n",
      "0.7254901960784313\n",
      "0.7941176470588235\n",
      "0.7745098039215687\n",
      "0.7745098039215687\n",
      "0.803921568627451\n",
      "0.7745098039215687\n",
      "0.7549019607843137\n",
      "0.8431372549019608\n",
      "0.7254901960784313\n",
      "0.7254901960784313\n",
      "0.7254901960784313\n",
      "0.7647058823529411\n",
      "0.8137254901960784\n",
      "0.7549019607843137\n",
      "0.7745098039215687\n",
      "0.7450980392156863\n",
      "0.7450980392156863\n",
      "0.8235294117647058\n",
      "0.8235294117647058\n",
      "0.7352941176470589\n",
      "0.8529411764705882\n",
      "0.7254901960784313\n",
      "0.7745098039215687\n",
      "0.7647058823529411\n",
      "0.7254901960784313\n",
      "0.7254901960784313\n",
      "0.7647058823529411\n",
      "0.7450980392156863\n",
      "0.8137254901960784\n",
      "0.803921568627451\n",
      "0.7745098039215687\n",
      "0.7450980392156863\n",
      "0.7549019607843137\n",
      "0.7843137254901961\n",
      "0.7941176470588235\n",
      "0.7450980392156863\n",
      "0.7941176470588235\n",
      "0.7352941176470589\n",
      "0.7549019607843137\n",
      "0.7156862745098039\n",
      "0.7450980392156863\n",
      "0.8235294117647058\n",
      "0.7941176470588235\n",
      "0.7745098039215687\n",
      "0.7352941176470589\n",
      "0.7254901960784313\n",
      "0.7058823529411765\n",
      "0.7058823529411765\n",
      "0.7450980392156863\n"
     ]
    }
   ],
   "source": [
    "values_svm_bi = []\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = create_params(clean_data, data_labels, 2)\n",
    "    values_svm_bi.append(svm_classification(X_train, X_test, y_train, y_test))\n",
    "print(mean(values_svm_bi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Os resultados estÃ£o de acordo com a teoria: os bigramas perfomam melhor, assim como SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "\n",
    "-- http://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1012 points : 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9812252964426877"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "ngram = 2\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    "    ngram_range=(ngram, ngram)\n",
    ")\n",
    "\n",
    "features = vectorizer.fit_transform(\n",
    "    data\n",
    ")\n",
    "\n",
    "features_nd = features.toarray()\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(features_nd, data_labels).predict(features_nd)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (features_nd.shape[0],(data_labels != y_pred).sum()))\n",
    "\n",
    "gnb.score(features_nd, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1012 points : 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9812252964426877"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "ngram = 2\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    "    stop_words = stopwords,\n",
    "    ngram_range=(ngram, ngram)\n",
    ")\n",
    "\n",
    "features = vectorizer.fit_transform(\n",
    "    data\n",
    ")\n",
    "\n",
    "features_nd = features.toarray()\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(features_nd, data_labels).predict(features_nd)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (features_nd.shape[0],(data_labels != y_pred).sum()))\n",
    "\n",
    "gnb.score(features_nd, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1012 points : 29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9713438735177866"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram = 1\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    "    stop_words = stopwords,\n",
    "    ngram_range=(ngram, ngram)\n",
    ")\n",
    "\n",
    "features = vectorizer.fit_transform(\n",
    "    data\n",
    ")\n",
    "\n",
    "features_nd = features.toarray()\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(features_nd, data_labels).predict(features_nd)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (features_nd.shape[0],(data_labels != y_pred).sum()))\n",
    "\n",
    "gnb.score(features_nd, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1012 points : 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9723320158102767"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram = 1\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    "    ngram_range=(ngram, ngram)\n",
    ")\n",
    "\n",
    "features = vectorizer.fit_transform(\n",
    "    data\n",
    ")\n",
    "\n",
    "features_nd = features.toarray()\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(features_nd, data_labels).predict(features_nd)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (features_nd.shape[0],(data_labels != y_pred).sum()))\n",
    "\n",
    "gnb.score(features_nd, data_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Para evitar overfitting. Mais aqui: http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "\n",
    "SVC\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77832512, 0.77832512, 0.77722772, 0.77722772, 0.77722772])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "ngram = 2\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    "    ngram_range=(ngram, ngram)\n",
    ")\n",
    "\n",
    "features = vectorizer.fit_transform(\n",
    "    data\n",
    ")\n",
    "\n",
    "features_nd = features.toarray()\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=0.0001)\n",
    "scores = cross_val_score(clf, features_nd, data_labels, cv=5)\n",
    "scores \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77832512 0.77832512 0.77722772 0.77722772 0.77722772]\n",
      "Accuracy: 0.78 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='rbf', C=0.0001)\n",
    "scores = cross_val_score(clf, features_nd, data_labels, cv=5)\n",
    "print(scores) \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77832512 0.77832512 0.77722772 0.77722772 0.77722772]\n",
      "Accuracy: 0.78 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='rbf', C=1)\n",
    "scores = cross_val_score(clf, features_nd, data_labels, cv=5)\n",
    "print(scores) \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77832512 0.77832512 0.77722772 0.77722772 0.77722772]\n",
      "Accuracy: 0.78 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=0.0001)\n",
    "scores = cross_val_score(clf, features_nd, data_labels, cv=5)\n",
    "print(scores) \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79310345 0.77832512 0.76732673 0.78217822 0.78217822]\n",
      "Accuracy: 0.78 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, features_nd, data_labels, cv=5)\n",
    "print(scores) \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunando os hiperparÃ¢metros\n",
    "\n",
    "Grid search:: \n",
    "\n",
    "-- http://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n",
    "-- https://medium.com/@aneesha/svm-parameter-tuning-in-scikit-learn-using-gridsearchcv-2413c02125a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
